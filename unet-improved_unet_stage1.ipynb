{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    " \n",
    "import random\n",
    " \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load crop data pairs\n",
    "\n",
    "# Paths to your data.\n",
    "pos_path = './archive/cropped/CFD_background_region/'\n",
    "neg_path = './archive/cropped/CFD_background_region/'\n",
    "img_size = (112, 112)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data_pairs(pos_path, neg_path):\n",
    "    pos_images= []  # List to store the images\n",
    "    neg_images = []   # List to store the masks\n",
    "    pairList = []\n",
    "    files = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(pos_path):\n",
    "        if count > 36:\n",
    "            break\n",
    "        pos_img_path = pos_path + filename\n",
    "        neg_img_path = neg_path + filename  # Assuming mask has same filename\n",
    "        # print(img_path)\n",
    "        # Load and preprocess the image\n",
    "        pos_img = cv2.imread(pos_img_path)\n",
    "        neg_img = cv2.imread(neg_img_path)\n",
    "        # print(img.size())\n",
    "    #   if not img or not mask:\n",
    "    #         continue\n",
    "        pos_img, neg_img = pos_img/255.0,  neg_img/255.0\n",
    "        # img = cv2.resize(img, img_size) / 255.0\n",
    "        # # Load and preprocess the mask using OpenCV\n",
    "        # mask = cv2.resize(mask, img_size) / 255.0\n",
    " \n",
    "        # for i in range(4):\n",
    "        #     for j in range(4):\n",
    "        #         img_part = pos_img[i * 112: (i + 1) * 112,\n",
    "        #                     j * 112: (j + 1) * 112]\n",
    "        #         mask_part = neg_img[i * 112: (i + 1) * 112,\n",
    "        #                     j * 112: (j + 1) * 112]\n",
    "        pos_images.append(pos_img)\n",
    "        neg_images.append(neg_img)\n",
    "        files.append(filename)\n",
    "         \n",
    "        count += 1\n",
    " \n",
    "    return np.array(pos_images), np.array(neg_images)\n",
    " \n",
    "\n",
    "pos_img, neg_img = load_data_pairs(pos_path, neg_path)\n",
    "pos_ds = tf.data.Dataset.from_tensor_slices(pos_img)\n",
    "neg_ds = tf.data.Dataset.from_tensor_slices(neg_img)\n",
    "\n",
    "# Zip the positive and negative datasets together to create pairs\n",
    "train_dataset = tf.data.Dataset.zip((pos_ds, neg_ds))\n",
    "\n",
    "# Shuffle and batch the dataset as needed\n",
    "buffer_size = len(pos_img)  # This should be the number of images, can be replaced with a suitable buffer size\n",
    "batch_size = 2  # Define your batch size as needed\n",
    "train_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "\n",
    "# Prefetch data for performance optimization\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improved unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "# def residual_block(input_tensor, num_filters):\n",
    "#     x = layers.Conv2D(num_filters, 3, padding=\"same\")(input_tensor)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "\n",
    "#     x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "\n",
    "#     # Add the input_tensor (residual connection) to the output of the convolution block\n",
    "#     x = layers.add([x, input_tensor])\n",
    "#     x = layers.ReLU()(x)\n",
    "\n",
    "#     return x\n",
    "def residual_block(input_tensor, num_filters):\n",
    "    # Check if a projection shortcut is needed\n",
    "    input_shape = input_tensor.shape[-1]\n",
    "    if input_shape != num_filters:\n",
    "        shortcut = layers.Conv2D(num_filters, (1, 1), strides=(1, 1), padding='same')(input_tensor)\n",
    "    else:\n",
    "        shortcut = input_tensor\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add the shortcut (which has been adjusted if necessary) to the output of the convolution block\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "# Attention gate\n",
    "def attention_gate(input_tensor, gate_tensor, num_filters):\n",
    "    # Resize gate tensor to match the input_tensor shape using a 1x1 convolution\n",
    "    gate_resized = layers.Conv2D(num_filters, 1, padding='same')(gate_tensor)\n",
    "    gate_resized = layers.BatchNormalization()(gate_resized)\n",
    "    gate_resized = layers.ReLU()(gate_resized)\n",
    "\n",
    "    # Add the gate to the input_tensor\n",
    "    x = layers.add([input_tensor, gate_resized])\n",
    "    attention = layers.Conv2D(num_filters, 1, padding='same', activation='sigmoid')(x)\n",
    "    x = layers.multiply([input_tensor, attention])\n",
    "\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    x = residual_block(input_tensor, num_filters)\n",
    "    return x\n",
    "\n",
    "def encoder_block(input_tensor, num_filters, block_name):\n",
    "    x = layers.Conv2D(num_filters, (3, 3), activation='relu', padding='same', name=f\"{block_name}_conv\")(input_tensor)\n",
    "    x = layers.BatchNormalization(name=f\"{block_name}_bn\")(x)\n",
    "    p = layers.MaxPooling2D((2, 2), name=f\"{block_name}_pool\")(x)\n",
    "    return x, p\n",
    "# def encoder_block(input_tensor, num_filters):\n",
    "#     x = conv_block(input_tensor, num_filters)\n",
    "#     p = layers.MaxPooling2D((2, 2))(x)\n",
    "#     return x, p\n",
    "\n",
    "# Modify the decoder_block to include an attention gate\n",
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_tensor)\n",
    "\n",
    "    # Apply attention gate to the skip features before concatenating\n",
    "    attention_skipped = attention_gate(skip_features, x, num_filters)\n",
    "\n",
    "    # Concatenate upsampled input with the attention modified skip features\n",
    "    x = layers.concatenate([x, attention_skipped])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "# def build_unet(input_shape):\n",
    "#     inputs = layers.Input(input_shape)\n",
    "\n",
    "#     # Encoder\n",
    "#     s1, p1 = encoder_block(inputs, 64)\n",
    "#     s2, p2 = encoder_block(p1, 128)\n",
    "#     s3, p3 = encoder_block(p2, 256)\n",
    "#     s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "#     # Bridge\n",
    "#     b1 = conv_block(p4, 1024)\n",
    "\n",
    "#     # Decoder\n",
    "#     d1 = decoder_block(b1, s4, 512)\n",
    "#     d2 = decoder_block(d1, s3, 256)\n",
    "#     d3 = decoder_block(d2, s2, 128)\n",
    "#     d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "#     # Output\n",
    "#     outputs = layers.Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "# input_shape = (112, 112, 3)\n",
    "# model = build_unet(input_shape)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_encoder(input_shape):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x, _ = encoder_block(inputs, 64)\n",
    "#     x, _ = encoder_block(x, 128)\n",
    "#     x, _ = encoder_block(x, 256)\n",
    "#     x, _ = encoder_block(x, 512)\n",
    "#     x = conv_block(x, 1024)\n",
    "#     x = layers.GlobalAveragePooling2D()(x)\n",
    "#     return keras.Model(inputs, x, name=\"encoder\")\n",
    "def build_encoder(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    s1, p1 = encoder_block(inputs, 64, \"pre1\")\n",
    "    s2, p2 = encoder_block(p1, 128, \"pre2\")\n",
    "    s3, p3 = encoder_block(p2, 256, \"pre3\")\n",
    "    s4, p4 = encoder_block(p3, 512, \"pre4\")\n",
    " \n",
    "    x = layers.GlobalAveragePooling2D()(p4)\n",
    "    return keras.Model(inputs, x, name=\"encoder\")\n",
    "\n",
    "# Define the projection head using the functional API\n",
    "def build_projection_head(encoder):\n",
    "    inputs = layers.Input(shape=(512,))\n",
    "    x = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    outputs = layers.Dense(256)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"projection_head\")\n",
    "\n",
    "# Get the models\n",
    "input_shape = (112, 112, 3)\n",
    "encoder_model = build_encoder(input_shape)\n",
    "projection_head_model = build_projection_head(encoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(z1, z2):\n",
    "    z1 = tf.math.l2_normalize(z1, axis=1)\n",
    "    z2 = tf.math.l2_normalize(z2, axis=1)\n",
    "    return tf.matmul(z1, z2, transpose_b=True)\n",
    "\n",
    "def contrastive_loss(z_crack, z_bg, other_proj, temperature=0.5):\n",
    "    # Normalize the input projections\n",
    "    z_crack_norm = tf.math.l2_normalize(z_crack, axis=1)\n",
    "    z_bg_norm = tf.math.l2_normalize(z_bg, axis=1)\n",
    "    other_proj_norm = tf.math.l2_normalize(other_proj, axis=1)\n",
    " \n",
    "    # Compute the positive similarity scores\n",
    "    pos_sim = tf.reduce_sum(z_crack_norm * z_bg_norm, axis=1, keepdims=True)\n",
    "    exp_pos_sim = tf.exp(pos_sim / temperature)\n",
    "\n",
    "    # Compute the negative similarity scores\n",
    "    neg_sim_z_crack = tf.matmul(z_crack_norm, other_proj_norm, transpose_b=True)\n",
    "    neg_sim_z_bg = tf.matmul(z_bg_norm, other_proj_norm, transpose_b=True)\n",
    "\n",
    "    # Sum the exponentials of the negative similarities\n",
    "    sum_exp_neg_sim_z_crack = tf.reduce_sum(tf.exp(neg_sim_z_crack / temperature), axis=1, keepdims=True)\n",
    "    sum_exp_neg_sim_z_bg = tf.reduce_sum(tf.exp(neg_sim_z_bg / temperature), axis=1, keepdims=True)\n",
    "\n",
    "    # Calculate the loss for the positive pair with respect to all other projections\n",
    "    loss_z_crack = -tf.math.log(exp_pos_sim / (sum_exp_neg_sim_z_crack + exp_pos_sim + 1e-9)) # Add epsilon for numerical stability\n",
    "    loss_z_bg = -tf.math.log(exp_pos_sim / (sum_exp_neg_sim_z_bg + exp_pos_sim + 1e-9))\n",
    "\n",
    "    # Combine the losses for both members of the positive pair\n",
    "    loss_per_pair = loss_z_crack + loss_z_bg\n",
    "\n",
    "    # Average the loss across all positive pairs\n",
    "    loss = tf.reduce_mean(loss_per_pair)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19/19 [00:04<00:00,  3.92it/s, loss=-0.0000]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tf.keras.Input(shape=(112, 112, 3))\n",
    "\n",
    "# Forward pass through the encoder and projection head\n",
    "encoder = build_encoder(input_shape=(112, 112, 3))\n",
    "encoder_output_shape = encoder.output_shape[1]\n",
    "projection_head = build_projection_head(encoder_output_shape)\n",
    "\n",
    "encoded = encoder(inputs)\n",
    "projected = projection_head(encoded)\n",
    "\n",
    "# Define the Keras model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=projected)\n",
    "\n",
    "# Compile the model with a dummy optimizer and loss since we are using a custom training loop\n",
    "model.compile(optimizer='adam', loss=None)\n",
    "num_epochs = 1\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    pbar = tqdm(total=len(train_dataset), desc='Training', leave=True)\n",
    "    \n",
    "    for step, (images_crack, images_bg) in enumerate(train_dataset):\n",
    "        # Open the GradientTape context to capture gradients\n",
    "        with tf.GradientTape() as tape: \n",
    "            # Compute all projections first\n",
    "            proj_crack = model(images_crack, training=True)  # Projections of crack images\n",
    "            proj_bg = model(images_bg, training=True)  # Projections of background images\n",
    "            all_projections = tf.concat([proj_crack, proj_bg], axis=0)\n",
    "\n",
    "            # Calculate the loss for each image\n",
    "            loss_values = []\n",
    "            for i in range(tf.shape(proj_crack)[0]):\n",
    "                # Positive pair for the current sample\n",
    "                pos_pair = tf.concat([proj_crack[i:i+1], proj_bg[i:i+1]], axis=0)\n",
    "                other_bg = tf.concat([proj_bg[0:i], proj_bg[i+1:]], axis=0)\n",
    "                other_crack = tf.concat([proj_crack[0:i], proj_crack[i+1:]], axis=0)\n",
    "                other_pj = tf.concat([other_bg[:], other_crack[:]], axis = 0)\n",
    "                # Compute the loss for the current pair against all other projections\n",
    "                loss_value = contrastive_loss(pos_pair[0:1], pos_pair[1:2], other_pj, temperature=0.5)\n",
    "                loss_values.append(loss_value)\n",
    "\n",
    "            # Calculate the mean of the loss values for gradient update\n",
    "            batch_loss = tf.reduce_mean(tf.stack(loss_values))\n",
    "\n",
    "        # Calculate gradients and update model weights\n",
    "        gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(loss=f\"{batch_loss.numpy():.4f}\")\n",
    "\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layers = [layer for layer in encoder_model.layers if 'conv' in layer.name or 'pooling' in layer.name]\n",
    "# model_encoder_layers = [layer for layer in model.layers if 'conv' in layer.name or 'pooling' in layer.name][:len(encoder_layers)]\n",
    "# for unet_layer, encoder_layer in zip(model_encoder_layers, encoder_layers):\n",
    "#     unet_layer.set_weights(encoder_layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_tensor)\n",
    "    attention_skipped = attention_gate(skip_features, x, num_filters)  # Apply attention gate\n",
    "    x = layers.concatenate([x, attention_skipped])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "# Function to build the entire U-Net model\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64, \"encoder1\")\n",
    "    s2, p2 = encoder_block(p1, 128, \"encoder2\")\n",
    "    s3, p3 = encoder_block(p2, 256, \"encoder3\")\n",
    "    s4, p4 = encoder_block(p3, 512, \"encoder4\")\n",
    "\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    " \n",
    "\n",
    "# Build the U-Net model\n",
    "input_shape = (112, 112, 3)\n",
    "unet_model = build_unet(input_shape)\n",
    "for layer in unet_model.layers:\n",
    "    if 'enoder' in layer.name:  # Only load weights for the encoder layers\n",
    "        layer_name = \"pre\" + layer.name[-1]  # Assumes layer names are like \"block1_conv\"\n",
    "        layer_weights = encoder.get_layer(layer_name).get_weights()\n",
    "        layer.set_weights(layer_weights)\n",
    "\n",
    "# Compile the U-Net model\n",
    "unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to your data.\n",
    "images_path = './archive/crack_segmentation_dataset/train/images/'\n",
    "masks_path = './archive/crack_segmentation_dataset/train/masks/'\n",
    "img_size = (112, 112)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(images_path, masks_path, img_size):\n",
    "    images = []  # List to store the images\n",
    "    masks = []   # List to store the masks\n",
    "    path = ''\n",
    "    files = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(images_path):\n",
    "        if count > 500:\n",
    "            break\n",
    "        img_path = images_path + filename\n",
    "        mask_path = masks_path + filename  # Assuming mask has same filename\n",
    "        # print(img_path)\n",
    "        # Load and preprocess the image\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # print(img.size())\n",
    "    #   if not img or not mask:\n",
    "    #         continue\n",
    "        img, mask = img/255.0, mask/255.0\n",
    "        # img = cv2.resize(img, img_size) / 255.0\n",
    "        # # Load and preprocess the mask using OpenCV\n",
    "        # mask = cv2.resize(mask, img_size) / 255.0\n",
    " \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                img_part = img[i * 112: (i + 1) * 112,\n",
    "                            j * 112: (j + 1) * 112]\n",
    "                mask_part = mask[i * 112: (i + 1) * 112,\n",
    "                            j * 112: (j + 1) * 112]\n",
    "                images.append(img_part)\n",
    "                masks.append(mask_part)\n",
    "        files.append(filename)\n",
    "         \n",
    "        count += 1\n",
    "    print(files)\n",
    "    return np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(images_path, masks_path, img_size):\n",
    "    images = []  # List to store the images\n",
    "    masks = []   # List to store the masks\n",
    "    path = ''\n",
    "    count = 0\n",
    "    print()\n",
    "    imgList = os.listdir(images_path)\n",
    "    files = []\n",
    "    for i in range(len(imgList) - 1, -1, -1):\n",
    "        filename = imgList[i]\n",
    "    # for filename in os.listdir(images_path):\n",
    "        if count > 100:\n",
    "            break\n",
    "\n",
    "        img_path = images_path + filename\n",
    "        mask_path = masks_path + filename  # Assuming mask has same filename\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img, mask = img/255.0, mask/255.0\n",
    "\n",
    "        i, j = random.randint(0, 3), random.randint(0, 3)\n",
    "        img_part = img[i * 112: (i + 1) * 112,\n",
    "                    j * 112: (j + 1) * 112]\n",
    "        mask_part = mask[i * 112: (i + 1) * 112,\n",
    "                    j * 112: (j + 1) * 112]\n",
    "        images.append(img_part)\n",
    "        masks.append(mask_part)\n",
    "\n",
    "        files.append(filename)\n",
    "        count += 1\n",
    "    print(files)\n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CFD_002.jpg', 'CFD_003.jpg', 'CFD_004.jpg', 'CFD_005.jpg', 'CFD_006.jpg', 'CFD_008.jpg', 'CFD_009.jpg', 'CFD_010.jpg', 'CFD_012.jpg', 'CFD_015.jpg', 'CFD_016.jpg', 'CFD_017.jpg', 'CFD_018.jpg', 'CFD_020.jpg', 'CFD_021.jpg', 'CFD_022.jpg', 'CFD_023.jpg', 'CFD_024.jpg', 'CFD_025.jpg', 'CFD_026.jpg', 'CFD_027.jpg', 'CFD_028.jpg', 'CFD_029.jpg', 'CFD_030.jpg', 'CFD_031.jpg', 'CFD_032.jpg', 'CFD_033.jpg', 'CFD_034.jpg', 'CFD_035.jpg', 'CFD_036.jpg', 'CFD_038.jpg', 'CFD_039.jpg', 'CFD_041.jpg', 'CFD_042.jpg', 'CFD_043.jpg', 'CFD_044.jpg', 'CFD_045.jpg', 'CFD_046.jpg', 'CFD_048.jpg', 'CFD_049.jpg', 'CFD_050.jpg', 'CFD_051.jpg', 'CFD_052.jpg', 'CFD_053.jpg', 'CFD_054.jpg', 'CFD_055.jpg', 'CFD_056.jpg', 'CFD_057.jpg', 'CFD_058.jpg', 'CFD_059.jpg', 'CFD_060.jpg', 'CFD_061.jpg', 'CFD_062.jpg', 'CFD_063.jpg', 'CFD_064.jpg', 'CFD_065.jpg', 'CFD_066.jpg', 'CFD_067.jpg', 'CFD_068.jpg', 'CFD_069.jpg', 'CFD_071.jpg', 'CFD_072.jpg', 'CFD_073.jpg', 'CFD_074.jpg', 'CFD_075.jpg', 'CFD_076.jpg', 'CFD_077.jpg', 'CFD_078.jpg', 'CFD_079.jpg', 'CFD_081.jpg', 'CFD_083.jpg', 'CFD_085.jpg', 'CFD_086.jpg', 'CFD_087.jpg', 'CFD_089.jpg', 'CFD_090.jpg', 'CFD_091.jpg', 'CFD_092.jpg', 'CFD_093.jpg', 'CFD_094.jpg', 'CFD_095.jpg', 'CFD_096.jpg', 'CFD_097.jpg', 'CFD_098.jpg', 'CFD_099.jpg', 'CFD_100.jpg', 'CFD_101.jpg', 'CFD_102.jpg', 'CFD_103.jpg', 'CFD_104.jpg', 'CFD_105.jpg', 'CFD_106.jpg', 'CFD_107.jpg', 'CFD_110.jpg', 'CFD_112.jpg', 'CFD_113.jpg', 'CFD_114.jpg', 'CFD_115.jpg', 'CFD_116.jpg', 'CFD_117.jpg', 'CRACK500_20160222_080850_1281_361.jpg', 'CRACK500_20160222_080850_1281_721.jpg', 'CRACK500_20160222_080850_1921_1.jpg', 'CRACK500_20160222_080850_1921_361.jpg', 'CRACK500_20160222_080850_1_361.jpg', 'CRACK500_20160222_080850_641_361.jpg', 'CRACK500_20160222_080850_641_721.jpg', 'CRACK500_20160222_080933_361_1281.jpg', 'CRACK500_20160222_080933_361_1921.jpg', 'CRACK500_20160222_080933_361_641.jpg', 'CRACK500_20160222_080933_721_1.jpg', 'CRACK500_20160222_080933_721_1921.jpg', 'CRACK500_20160222_080933_721_641.jpg', 'CRACK500_20160222_081011_1281_721.jpg', 'CRACK500_20160222_081011_1921_721.jpg', 'CRACK500_20160222_081011_1_361.jpg', 'CRACK500_20160222_081011_1_721.jpg', 'CRACK500_20160222_081011_641_361.jpg', 'CRACK500_20160222_081011_641_721.jpg', 'CRACK500_20160222_081031_1281_361.jpg', 'CRACK500_20160222_081031_1921_361.jpg', 'CRACK500_20160222_081031_1921_721.jpg', 'CRACK500_20160222_081031_1_721.jpg', 'CRACK500_20160222_081031_641_721.jpg', 'CRACK500_20160222_081102_1281_361.jpg', 'CRACK500_20160222_081102_1281_721.jpg', 'CRACK500_20160222_081102_1921_1.jpg', 'CRACK500_20160222_081102_1921_1081.jpg', 'CRACK500_20160222_081102_1921_361.jpg', 'CRACK500_20160222_081102_641_361.jpg', 'CRACK500_20160222_081102_641_721.jpg', 'CRACK500_20160222_081111_1281_361.jpg', 'CRACK500_20160222_081111_1281_721.jpg', 'CRACK500_20160222_081111_1921_721.jpg', 'CRACK500_20160222_081111_1_1.jpg', 'CRACK500_20160222_081111_1_361.jpg', 'CRACK500_20160222_081113_1281_361.jpg', 'CRACK500_20160222_081113_1281_721.jpg', 'CRACK500_20160222_081113_1_1.jpg', 'CRACK500_20160222_081113_1_361.jpg', 'CRACK500_20160222_081113_1_721.jpg', 'CRACK500_20160222_081113_641_361.jpg', 'CRACK500_20160222_081113_641_721.jpg', 'CRACK500_20160222_081839_1281_1.jpg', 'CRACK500_20160222_081839_1281_1081.jpg', 'CRACK500_20160222_081839_1281_361.jpg', 'CRACK500_20160222_081839_1281_721.jpg', 'CRACK500_20160222_081839_1921_361.jpg', 'CRACK500_20160222_081839_641_721.jpg', 'CRACK500_20160222_081908_1281_1.jpg', 'CRACK500_20160222_081908_1281_1081.jpg', 'CRACK500_20160222_081908_1281_361.jpg', 'CRACK500_20160222_081908_1281_721.jpg', 'CRACK500_20160222_081908_1_1.jpg', 'CRACK500_20160222_081908_1_1081.jpg', 'CRACK500_20160222_081908_1_361.jpg', 'CRACK500_20160222_081908_1_721.jpg', 'CRACK500_20160222_081908_641_361.jpg', 'CRACK500_20160222_082414_1281_1081.jpg', 'CRACK500_20160222_082414_1281_361.jpg', 'CRACK500_20160222_082414_1281_721.jpg', 'CRACK500_20160222_082414_1_361.jpg', 'CRACK500_20160222_082414_641_361.jpg', 'CRACK500_20160222_114759_1281_1.jpg', 'CRACK500_20160222_114759_1281_1081.jpg', 'CRACK500_20160222_114759_1281_361.jpg', 'CRACK500_20160222_114759_1281_721.jpg', 'CRACK500_20160222_114759_1921_1.jpg', 'CRACK500_20160222_114759_1921_1081.jpg', 'CRACK500_20160222_114759_1_1081.jpg', 'CRACK500_20160222_114759_1_361.jpg', 'CRACK500_20160222_114759_1_721.jpg', 'CRACK500_20160222_114759_641_1.jpg', 'CRACK500_20160222_114759_641_1081.jpg', 'CRACK500_20160222_114759_641_721.jpg', 'CRACK500_20160222_114806_1281_1.jpg', 'CRACK500_20160222_114806_1281_1081.jpg', 'CRACK500_20160222_114806_1281_361.jpg', 'CRACK500_20160222_114806_1281_721.jpg', 'CRACK500_20160222_115219_1281_361.jpg', 'CRACK500_20160222_115219_1921_1081.jpg', 'CRACK500_20160222_115219_641_1.jpg', 'CRACK500_20160222_115219_641_361.jpg', 'CRACK500_20160222_115224_1281_1.jpg', 'CRACK500_20160222_115224_1281_721.jpg', 'CRACK500_20160222_115224_1921_1081.jpg', 'CRACK500_20160222_115224_641_1.jpg', 'CRACK500_20160222_115224_641_1081.jpg', 'CRACK500_20160222_115224_641_361.jpg', 'CRACK500_20160222_115224_641_721.jpg', 'CRACK500_20160222_115233_1281_361.jpg', 'CRACK500_20160222_115233_1281_721.jpg', 'CRACK500_20160222_115233_1921_721.jpg', 'CRACK500_20160222_115233_1_361.jpg', 'CRACK500_20160222_115233_1_721.jpg', 'CRACK500_20160222_115233_641_361.jpg', 'CRACK500_20160222_115305_1921_1081.jpg', 'CRACK500_20160222_115305_641_1081.jpg', 'CRACK500_20160222_115305_641_361.jpg', 'CRACK500_20160222_115324_1281_1.jpg', 'CRACK500_20160222_115324_1281_361.jpg', 'CRACK500_20160222_115324_1_1.jpg', 'CRACK500_20160222_115324_641_361.jpg', 'CRACK500_20160222_115324_641_721.jpg', 'CRACK500_20160222_115519_641_1.jpg', 'CRACK500_20160222_115519_641_1081.jpg', 'CRACK500_20160222_115519_641_361.jpg', 'CRACK500_20160222_115620_1281_1.jpg', 'CRACK500_20160222_115620_641_1081.jpg', 'CRACK500_20160222_115620_641_721.jpg', 'CRACK500_20160222_115714_1281_1081.jpg', 'CRACK500_20160222_115714_1281_721.jpg', 'CRACK500_20160222_115714_641_1.jpg', 'CRACK500_20160222_115714_641_361.jpg', 'CRACK500_20160222_115738_1281_1.jpg', 'CRACK500_20160222_115738_1281_1081.jpg', 'CRACK500_20160222_115738_1281_361.jpg', 'CRACK500_20160222_115738_1281_721.jpg', 'CRACK500_20160222_115738_641_1081.jpg', 'CRACK500_20160222_115805_641_1.jpg', 'CRACK500_20160222_115805_641_1081.jpg', 'CRACK500_20160222_115805_641_361.jpg', 'CRACK500_20160222_115805_641_721.jpg', 'CRACK500_20160222_115828_641_1081.jpg', 'CRACK500_20160222_115828_641_361.jpg', 'CRACK500_20160222_115828_641_721.jpg', 'CRACK500_20160222_115833_1281_1081.jpg', 'CRACK500_20160222_115833_1281_361.jpg', 'CRACK500_20160222_115833_1281_721.jpg', 'CRACK500_20160222_115833_641_1.jpg', 'CRACK500_20160222_115837_1281_1.jpg', 'CRACK500_20160222_115837_1281_1081.jpg', 'CRACK500_20160222_115837_1281_361.jpg', 'CRACK500_20160222_115837_1281_721.jpg', 'CRACK500_20160222_115843_1281_1.jpg', 'CRACK500_20160222_115843_1281_721.jpg', 'CRACK500_20160222_115843_641_1081.jpg', 'CRACK500_20160222_115847_1281_1.jpg', 'CRACK500_20160222_115847_641_1081.jpg', 'CRACK500_20160222_115847_641_721.jpg', 'CRACK500_20160222_115858_641_1.jpg', 'CRACK500_20160222_115858_641_1081.jpg', 'CRACK500_20160222_115858_641_361.jpg', 'CRACK500_20160222_115858_641_721.jpg', 'CRACK500_20160222_163930_1281_361.jpg', 'CRACK500_20160222_163930_1281_721.jpg', 'CRACK500_20160222_163930_1921_1081.jpg', 'CRACK500_20160222_163930_1921_721.jpg', 'CRACK500_20160222_163930_1_1.jpg', 'CRACK500_20160222_163930_1_1081.jpg', 'CRACK500_20160222_163930_1_361.jpg', 'CRACK500_20160222_163930_1_721.jpg', 'CRACK500_20160222_163930_641_1.jpg', 'CRACK500_20160222_163930_641_361.jpg', 'CRACK500_20160222_163930_641_721.jpg', 'CRACK500_20160222_163940_1281_721.jpg', 'CRACK500_20160222_163940_1_1.jpg', 'CRACK500_20160222_163940_1_721.jpg', 'CRACK500_20160222_163940_641_1.jpg', 'CRACK500_20160222_163940_641_361.jpg', 'CRACK500_20160222_163940_641_721.jpg', 'CRACK500_20160222_164000_1281_1.jpg', 'CRACK500_20160222_164000_1281_1081.jpg', 'CRACK500_20160222_164000_1281_361.jpg', 'CRACK500_20160222_164000_1281_721.jpg', 'CRACK500_20160222_164000_1921_1.jpg', 'CRACK500_20160222_164000_1921_361.jpg', 'CRACK500_20160222_164000_1_1.jpg', 'CRACK500_20160222_164000_1_361.jpg', 'CRACK500_20160222_164000_1_721.jpg', 'CRACK500_20160222_164000_641_1.jpg', 'CRACK500_20160222_164000_641_1081.jpg', 'CRACK500_20160222_164000_641_361.jpg', 'CRACK500_20160222_164000_641_721.jpg', 'CRACK500_20160222_164021_1281_1.jpg', 'CRACK500_20160222_164021_1281_361.jpg', 'CRACK500_20160222_164021_641_1081.jpg', 'CRACK500_20160222_164021_641_361.jpg', 'CRACK500_20160222_164021_641_721.jpg', 'CRACK500_20160222_164141_1281_361.jpg', 'CRACK500_20160222_164141_1281_721.jpg', 'CRACK500_20160222_164141_1921_1.jpg', 'CRACK500_20160222_164141_1921_1081.jpg', 'CRACK500_20160222_164141_1921_361.jpg', 'CRACK500_20160222_164141_1_1.jpg', 'CRACK500_20160222_164141_1_361.jpg', 'CRACK500_20160222_164141_1_721.jpg', 'CRACK500_20160222_164141_641_1.jpg', 'CRACK500_20160222_164141_641_1081.jpg', 'CRACK500_20160222_164141_641_361.jpg', 'CRACK500_20160222_164141_641_721.jpg', 'CRACK500_20160222_164822_1281_361.jpg', 'CRACK500_20160222_164822_1281_721.jpg', 'CRACK500_20160222_164822_641_1081.jpg', 'CRACK500_20160222_164822_641_721.jpg', 'CRACK500_20160222_164825_1281_1.jpg', 'CRACK500_20160222_164825_1281_361.jpg', 'CRACK500_20160222_164825_1281_721.jpg', 'CRACK500_20160222_164825_641_1081.jpg', 'CRACK500_20160222_164851_1281_1081.jpg', 'CRACK500_20160222_164851_641_1.jpg', 'CRACK500_20160222_164851_641_361.jpg', 'CRACK500_20160222_164851_641_721.jpg', 'CRACK500_20160222_164920_641_1.jpg', 'CRACK500_20160222_164920_641_361.jpg', 'CRACK500_20160222_164920_641_721.jpg', 'CRACK500_20160222_164936_641_1.jpg', 'CRACK500_20160222_164936_641_1081.jpg', 'CRACK500_20160222_164936_641_721.jpg', 'CRACK500_20160222_165012_1281_1.jpg', 'CRACK500_20160222_165012_641_1.jpg', 'CRACK500_20160222_165012_641_361.jpg', 'CRACK500_20160222_165012_641_721.jpg', 'CRACK500_20160222_165031_641_1.jpg', 'CRACK500_20160222_165031_641_1081.jpg', 'CRACK500_20160222_165031_641_361.jpg', 'CRACK500_20160222_165031_641_721.jpg', 'CRACK500_20160222_165037_1281_721.jpg', 'CRACK500_20160222_165037_1921_721.jpg', 'CRACK500_20160222_165037_1_721.jpg', 'CRACK500_20160222_165037_641_721.jpg', 'CRACK500_20160222_165218_1281_721.jpg', 'CRACK500_20160222_165218_1921_1081.jpg', 'CRACK500_20160222_165218_1921_721.jpg', 'CRACK500_20160222_165218_1_361.jpg', 'CRACK500_20160222_165218_641_361.jpg', 'CRACK500_20160222_165218_641_721.jpg', 'CRACK500_20160222_165225_1281_361.jpg', 'CRACK500_20160222_165225_1921_721.jpg', 'CRACK500_20160222_165225_1_361.jpg', 'CRACK500_20160222_165225_641_361.jpg', 'CRACK500_20160222_165256_1281_361.jpg', 'CRACK500_20160222_165256_1921_1081.jpg', 'CRACK500_20160222_165256_641_721.jpg', 'CRACK500_20160222_165402_1281_1.jpg', 'CRACK500_20160222_165402_1281_361.jpg', 'CRACK500_20160222_165402_641_1081.jpg', 'CRACK500_20160222_165402_641_721.jpg', 'CRACK500_20160222_165625_1281_361.jpg', 'CRACK500_20160222_165625_1921_361.jpg', 'CRACK500_20160222_165625_1921_721.jpg', 'CRACK500_20160222_165625_1_721.jpg', 'CRACK500_20160222_165625_641_361.jpg', 'CRACK500_20160222_165625_641_721.jpg', 'CRACK500_20160222_165715_1281_361.jpg', 'CRACK500_20160222_165715_1921_361.jpg', 'CRACK500_20160222_165715_1_721.jpg', 'CRACK500_20160222_165715_641_721.jpg', 'CRACK500_20160222_165909_1281_361.jpg', 'CRACK500_20160222_165909_1281_721.jpg', 'CRACK500_20160222_165909_1921_361.jpg', 'CRACK500_20160222_165909_1_1.jpg', 'CRACK500_20160222_165909_1_361.jpg', 'CRACK500_20160222_165909_641_361.jpg', 'CRACK500_20160222_165909_641_721.jpg', 'CRACK500_20160222_165919_1281_1.jpg', 'CRACK500_20160222_165919_1281_361.jpg', 'CRACK500_20160222_165919_1281_721.jpg', 'CRACK500_20160222_165919_1921_721.jpg', 'CRACK500_20160222_165919_1_361.jpg', 'CRACK500_20160222_165919_641_361.jpg', 'CRACK500_20160222_165919_641_721.jpg', 'CRACK500_20160222_165926_1281_361.jpg', 'CRACK500_20160222_165926_1921_721.jpg', 'CRACK500_20160222_165926_1_1081.jpg', 'CRACK500_20160222_165926_1_721.jpg', 'CRACK500_20160222_165926_641_1.jpg', 'CRACK500_20160222_165926_641_361.jpg', 'CRACK500_20160222_165926_641_721.jpg', 'CRACK500_20160222_165937_1281_361.jpg', 'CRACK500_20160222_165937_1281_721.jpg', 'CRACK500_20160222_165937_1921_721.jpg', 'CRACK500_20160222_165937_1_1081.jpg', 'CRACK500_20160222_165937_1_361.jpg', 'CRACK500_20160222_165937_1_721.jpg', 'CRACK500_20160222_165937_641_361.jpg', 'CRACK500_20160222_165937_641_721.jpg', 'CRACK500_20160222_165947_1281_721.jpg', 'CRACK500_20160222_165947_1_721.jpg', 'CRACK500_20160222_165947_641_1.jpg', 'CRACK500_20160222_165947_641_361.jpg', 'CRACK500_20160222_165947_641_721.jpg', 'CRACK500_20160222_165951_1281_1.jpg', 'CRACK500_20160222_165951_1281_1081.jpg', 'CRACK500_20160222_165951_1281_361.jpg', 'CRACK500_20160222_165951_1281_721.jpg', 'CRACK500_20160222_165951_1921_1081.jpg', 'CRACK500_20160222_165951_1921_361.jpg', 'CRACK500_20160222_165951_1921_721.jpg', 'CRACK500_20160222_165951_1_1.jpg', 'CRACK500_20160222_165951_641_1.jpg', 'CRACK500_20160222_165951_641_1081.jpg', 'CRACK500_20160222_165951_641_361.jpg', 'CRACK500_20160225_114503_1281_1.jpg', 'CRACK500_20160225_114503_1281_361.jpg', 'CRACK500_20160225_114503_1281_721.jpg', 'CRACK500_20160225_114503_1921_1.jpg', 'CRACK500_20160225_114503_1921_361.jpg', 'CRACK500_20160225_114503_1_1081.jpg', 'CRACK500_20160225_114503_1_721.jpg', 'CRACK500_20160225_114503_641_1.jpg', 'CRACK500_20160225_114503_641_1081.jpg', 'CRACK500_20160225_114503_641_361.jpg', 'CRACK500_20160225_114503_641_721.jpg', 'CRACK500_20160225_114514_1081_1.jpg', 'CRACK500_20160225_114514_1081_1281.jpg', 'CRACK500_20160225_114514_1081_1921.jpg', 'CRACK500_20160225_114514_1081_641.jpg', 'CRACK500_20160225_114514_1_1281.jpg', 'CRACK500_20160225_114514_1_641.jpg', 'CRACK500_20160225_114514_361_1.jpg', 'CRACK500_20160225_114514_361_1281.jpg', 'CRACK500_20160225_114514_361_641.jpg', 'CRACK500_20160225_114514_721_1.jpg', 'CRACK500_20160225_114531_1281_1.jpg', 'CRACK500_20160225_114531_1281_1081.jpg', 'CRACK500_20160225_114531_1281_361.jpg', 'CRACK500_20160225_114531_1281_721.jpg', 'CRACK500_20160225_114531_1921_1081.jpg', 'CRACK500_20160225_114531_1921_721.jpg', 'CRACK500_20160225_114531_641_1.jpg', 'CRACK500_20160225_114531_641_1081.jpg', 'CRACK500_20160225_114531_641_361.jpg', 'CRACK500_20160225_114531_641_721.jpg', 'CRACK500_20160225_114535_1281_1.jpg', 'CRACK500_20160225_114535_1281_361.jpg', 'CRACK500_20160225_114535_1281_721.jpg', 'CRACK500_20160225_114535_1921_721.jpg', 'CRACK500_20160225_114535_641_1.jpg', 'CRACK500_20160225_114535_641_361.jpg', 'CRACK500_20160225_114535_641_721.jpg', 'CRACK500_20160302_155848_1281_721.jpg', 'CRACK500_20160302_155848_1921_1.jpg', 'CRACK500_20160302_155848_1_721.jpg', 'CRACK500_20160302_155857_1281_361.jpg', 'CRACK500_20160302_155857_1281_721.jpg', 'CRACK500_20160302_155857_641_721.jpg', 'CRACK500_20160302_155914_361_1281.jpg', 'CRACK500_20160302_155914_361_1921.jpg', 'CRACK500_20160302_155914_361_641.jpg', 'CRACK500_20160302_155914_721_641.jpg', 'CRACK500_20160302_155919_1281_361.jpg', 'CRACK500_20160302_155919_1921_361.jpg', 'CRACK500_20160302_155919_641_721.jpg', 'CRACK500_20160302_155925_1281_1.jpg', 'CRACK500_20160302_155925_1921_361.jpg', 'CRACK500_20160302_155925_641_721.jpg', 'CRACK500_20160302_160033_1281_721.jpg', 'CRACK500_20160302_160033_1921_361.jpg', 'CRACK500_20160302_160033_1921_721.jpg', 'CRACK500_20160302_160033_1_361.jpg', 'CRACK500_20160302_160033_641_721.jpg', 'CRACK500_20160303_091825_1281_361.jpg', 'CRACK500_20160303_091825_1921_361.jpg', 'CRACK500_20160303_091825_1921_721.jpg', 'CRACK500_20160303_091825_1_1.jpg', 'CRACK500_20160303_091825_641_1.jpg', 'CRACK500_20160303_091825_641_1081.jpg', 'CRACK500_20160303_091825_641_361.jpg', 'CRACK500_20160303_091825_641_721.jpg', 'CRACK500_20160303_091830_1_1921.jpg', 'CRACK500_20160303_091830_361_1921.jpg', 'CRACK500_20160303_091830_721_1.jpg', 'CRACK500_20160303_091830_721_1281.jpg', 'CRACK500_20160303_091830_721_641.jpg', 'CRACK500_20160303_091837_361_1.jpg', 'CRACK500_20160303_091837_361_1281.jpg', 'CRACK500_20160303_091837_361_641.jpg', 'CRACK500_20160303_091837_721_1.jpg', 'CRACK500_20160303_091837_721_1281.jpg', 'CRACK500_20160303_091837_721_1921.jpg', 'CRACK500_20160303_092627_1281_361.jpg', 'CRACK500_20160303_092627_1921_361.jpg', 'CRACK500_20160303_092627_1921_721.jpg', 'CRACK500_20160303_092627_1_361.jpg', 'CRACK500_20160303_092627_1_721.jpg', 'CRACK500_20160303_092627_641_361.jpg', 'CRACK500_20160303_092806_1281_1.jpg', 'CRACK500_20160303_092806_1281_361.jpg', 'CRACK500_20160303_092806_1921_1081.jpg', 'CRACK500_20160303_092806_1921_721.jpg', 'CRACK500_20160303_092806_1_1081.jpg', 'CRACK500_20160303_092806_641_361.jpg', 'CRACK500_20160303_092806_641_721.jpg', 'CRACK500_20160303_092839_1921_1081.jpg', 'CRACK500_20160303_092839_1921_361.jpg', 'CRACK500_20160303_092839_1921_721.jpg', 'CRACK500_20160303_092839_1_721.jpg', 'CRACK500_20160303_093353_1281_361.jpg', 'CRACK500_20160303_093353_1281_721.jpg', 'CRACK500_20160303_093353_1_1081.jpg', 'CRACK500_20160303_093353_1_361.jpg', 'CRACK500_20160303_093353_1_721.jpg', 'CRACK500_20160303_093353_641_361.jpg', 'CRACK500_20160303_093400_1281_1081.jpg', 'CRACK500_20160303_093400_1281_361.jpg', 'CRACK500_20160303_093400_1281_721.jpg', 'CRACK500_20160303_093400_641_1081.jpg', 'CRACK500_20160303_093400_641_361.jpg', 'CRACK500_20160303_093527_1281_1.jpg', 'CRACK500_20160303_093527_1921_1.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "images, masks = load_data(images_path, masks_path, (112, 112))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8016, 112, 112, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, masks1 = images[:500], masks[:500]\n",
    "masks1 = np.expand_dims(masks1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Volker_DSC01710_725_197_1392_1586.jpg', 'Volker_DSC01710_608_6_1524_1817.jpg', 'Volker_DSC01710_521_437_1123_892.jpg', 'Volker_DSC01710_449_161_1824_1502.jpg', 'Volker_DSC01710_37_518_1605_1304.jpg', 'Volker_DSC01710_340_159_2049_1635.jpg', 'Volker_DSC01710_15_28_1388_1132.jpg', 'Volker_DSC01710_112_246_1993_1548.jpg', 'Volker_DSC01710_1087_22_1502_1742.jpg', 'Volker_DSC01710_1025_76_1293_1327.jpg', 'Volker_DSC01710_0_0_2736_1824.jpg', 'Volker_DSC01709_599_1337_1214_1373.jpg', 'Volker_DSC01709_525_1036_1256_1448.jpg', 'Volker_DSC01709_43_382_1337_1425.jpg', 'Volker_DSC01709_1_428_1791_1806.jpg', 'Volker_DSC01709_191_287_1561_2056.jpg', 'Volker_DSC01709_176_298_1435_1635.jpg', 'Volker_DSC01709_14_103_1566_1955.jpg', 'Volker_DSC01709_109_399_1680_2130.jpg', 'Volker_DSC01709_0_0_1824_2736.jpg', 'Volker_DSC01708_820_247_911_1098.jpg', 'Volker_DSC01708_7_339_1805_2017.jpg', 'Volker_DSC01708_6_308_1788_2155.jpg', 'Volker_DSC01708_3_1033_1692_1349.jpg', 'Volker_DSC01708_26_342_1730_1845.jpg', 'Volker_DSC01708_190_1369_1165_1016.jpg', 'Volker_DSC01708_162_428_1581_1891.jpg', 'Volker_DSC01708_14_25_1720_2189.jpg', 'Volker_DSC01708_146_565_1550_1877.jpg', 'Volker_DSC01708_0_0_1824_2736.jpg', 'Volker_DSC01707_70_122_1743_1638.jpg', 'Volker_DSC01707_631_1517_1144_1199.jpg', 'Volker_DSC01707_296_841_1466_1781.jpg', 'Volker_DSC01707_290_218_1225_1618.jpg', 'Volker_DSC01707_14_642_1208_1489.jpg', 'Volker_DSC01707_117_725_1538_1216.jpg', 'Volker_DSC01707_107_677_1669_2015.jpg', 'Volker_DSC01707_0_0_1824_2736.jpg', 'Volker_DSC01706_69_669_1577_1989.jpg', 'Volker_DSC01706_58_458_1678_1398.jpg', 'Volker_DSC01706_323_1714_1115_982.jpg', 'Volker_DSC01706_255_630_1554_1969.jpg', 'Volker_DSC01706_240_1123_1495_1323.jpg', 'Volker_DSC01706_183_735_1171_1164.jpg', 'Volker_DSC01706_131_863_1650_1722.jpg', 'Volker_DSC01706_121_1149_1463_1438.jpg', 'Volker_DSC01706_113_377_1590_1904.jpg', 'Volker_DSC01706_0_0_1824_2736.jpg', 'Volker_DSC01705_680_705_1083_950.jpg', 'Volker_DSC01705_446_137_1039_1159.jpg', 'Volker_DSC01705_389_1310_1098_920.jpg', 'Volker_DSC01705_221_527_1603_1965.jpg', 'Volker_DSC01705_181_365_1624_2023.jpg', 'Volker_DSC01705_108_1269_1575_1361.jpg', 'Volker_DSC01705_0_0_1824_2736.jpg', 'Volker_DSC01704_64_82_1215_1389.jpg', 'Volker_DSC01704_5_245_1818_2163.jpg', 'Volker_DSC01704_446_1296_1363_1130.jpg', 'Volker_DSC01704_365_495_1058_1256.jpg', 'Volker_DSC01704_35_556_1657_1753.jpg', 'Volker_DSC01704_229_314_1475_1939.jpg', 'Volker_DSC01704_226_39_928_1215.jpg', 'Volker_DSC01704_219_142_1156_1117.jpg', 'Volker_DSC01704_214_498_1312_1428.jpg', 'Volker_DSC01704_17_949_1389_1694.jpg', 'Volker_DSC01704_0_0_1824_2736.jpg', 'Volker_DSC01703_66_610_1432_1109.jpg', 'Volker_DSC01703_597_1216_1104_1108.jpg', 'Volker_DSC01703_574_1149_1207_1490.jpg', 'Volker_DSC01703_478_1348_1214_966.jpg', 'Volker_DSC01703_409_465_1334_1085.jpg', 'Volker_DSC01703_34_562_1772_1755.jpg', 'Volker_DSC01703_106_1066_1488_1583.jpg', 'Volker_DSC01703_0_0_1824_2736.jpg', 'Volker_DSC01702_80_579_1686_1357.jpg', 'Volker_DSC01702_53_496_1764_2075.jpg', 'Volker_DSC01702_485_615_1142_1023.jpg', 'Volker_DSC01702_353_632_1134_1498.jpg', 'Volker_DSC01702_293_97_1158_1277.jpg', 'Volker_DSC01702_14_320_1617_1798.jpg', 'Volker_DSC01702_148_426_1258_1569.jpg', 'Volker_DSC01702_128_622_1625_2108.jpg', 'Volker_DSC01702_0_0_1824_2736.jpg', 'Volker_DSC01701_84_515_1689_1933.jpg', 'Volker_DSC01701_676_1216_1050_1357.jpg', 'Volker_DSC01701_66_400_1209_1491.jpg', 'Volker_DSC01701_562_672_1127_1205.jpg', 'Volker_DSC01701_537_291_1230_1215.jpg', 'Volker_DSC01701_38_802_1782_1575.jpg', 'Volker_DSC01701_233_60_1569_1928.jpg', 'Volker_DSC01701_221_1280_1385_1324.jpg', 'Volker_DSC01701_212_870_1467_1795.jpg', 'Volker_DSC01701_0_0_1824_2736.jpg', 'Volker_DSC01700_82_197_1311_1539.jpg', 'Volker_DSC01700_403_348_1339_1574.jpg', 'Volker_DSC01700_3_1234_1751_1476.jpg', 'Volker_DSC01700_262_454_1451_1381.jpg', 'Volker_DSC01700_200_730_1473_1593.jpg', 'Volker_DSC01700_17_674_1805_1877.jpg', 'Volker_DSC01700_154_336_1381_1688.jpg', 'Volker_DSC01700_118_729_1178_1411.jpg']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your data\n",
    "imgval, maskval = load_test(images_path, masks_path, (112, 112))\n",
    "\n",
    "maskval = np.expand_dims(maskval, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 112, 112, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (500, 112, 112, 3)\n",
      "y_train shape: (500, 112, 112, 1)\n",
      "X_val shape: (101, 112, 112, 3)\n",
      "y_val shape: (101, 112, 112, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train, X_val, y_val = images1, masks1,imgval, maskval \n",
    "# y_train.shape\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 112, 112, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1[:200,:,:,:].shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 112, 112, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pretrain_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m dummy_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_samples, \u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mpretrain_model\u001b[49m\u001b[38;5;241m.\u001b[39mfit([pos_pairs, neg_pairs], dummy_y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pretrain_model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 images1.shape == (800, 112, 112, 3)\n",
    "num_samples = 200\n",
    " \n",
    "pos_pairs = images1[:num_samples,:,:,:] \n",
    "neg_pairs = images1[num_samples:2*num_samples,:,:,:] \n",
    "print(pos_pairs.shape)                  \n",
    " \n",
    "assert not np.isnan(pos_pairs).any() and not np.isnan(neg_pairs).any()\n",
    "  \n",
    " \n",
    "dummy_y = np.zeros((num_samples, 256))\n",
    "# 训练模型\n",
    "pretrain_model.fit([pos_pairs, neg_pairs], dummy_y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpretrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimages1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optree\\ops.py:521\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    519\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m    520\u001b[0m flat_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(func, \u001b[38;5;241m*\u001b[39mflat_args)\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "pretrain_model.fit([images1[:200,:,:,:], images1[200:400,:,:,:]], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9007 - loss: 0.2052 - val_accuracy: 0.9443 - val_loss: 0.5459\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 1s/step - accuracy: 0.9873 - loss: 0.0518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0263 - val_accuracy: 1.0000 - val_loss: 0.0042\n",
      "Epoch 3/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.9664 - loss: 0.0910 - val_accuracy: 0.9482 - val_loss: 0.1980\n",
      "Epoch 4/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9627 - loss: 0.0500 - val_accuracy: 0.9287 - val_loss: 0.1290\n",
      "Epoch 5/10\n",
      "\u001b[1m36/62\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9741 - loss: 0.0688"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create data generators with desired augmentation parameters\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "\n",
    "# Generate batches of augmented data from arrays (X_train and y_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=8)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=8)\n",
    "\n",
    "# unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Train the model using the fit_generator function\n",
    "history = unet_model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=len(X_train) // 8,  # Number of batches per epoch\n",
    "    epochs=10, \n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // 8  # Number of batches for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create data generators with desired augmentation parameters\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "\n",
    "# Generate batches of augmented data from arrays (X_train and y_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=8)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=8)\n",
    "for layer in model.layers[:4]:\n",
    "    layer.trainable = False\n",
    "    print(layer.name)\n",
    "    \n",
    "# unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Train the model using the fit_generator function\n",
    "# history = unet_model.fit(\n",
    "#     train_generator, \n",
    "#     steps_per_epoch=len(X_train) // 8,  # Number of batches per epoch\n",
    "#     epochs=10, \n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=len(X_val) // 8  # Number of batches for validation\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masg.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, image)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the image to a NumPy array with the expected shape\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43minput_image\u001b[49m)\n\u001b[0;32m     10\u001b[0m prediction\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'CFD_001.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.resize(image, (112, 112))  # Resize to match your model's input shape\n",
    "# image = image / 255.0  # Normalize the pixel values\n",
    "cv2.imwrite( \"asg.jpg\", image)\n",
    "# Convert the image to a NumPy array with the expected shape\n",
    "# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
    "prediction = model.predict(input_image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mprediction\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      6\u001b[0m prediction_2d \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction *= 255\n",
    "\n",
    "prediction_2d = prediction.squeeze()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(prediction_2d, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
