{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras import metrics\n",
    "import random\n",
    " \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = layers.concatenate([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (112, 112, 3)  # Adjust size and channels according to your dataset\n",
    "model = build_unet(input_shape)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "optimizer1 = tf.keras.optimizers.Adam(learning_rate=0.15) \n",
    "# unet_model_retrain.compile(optimizer=optimizer1 ,\n",
    "#                 #    loss=dice_loss,\n",
    "#                    loss = 'binary_crossentropy',\n",
    "#                    metrics=[metrics.Precision(),\n",
    "#                             metrics.Recall(),\n",
    "#                             metrics.FalsePositives(),\n",
    "#                             metrics.FalseNegatives(),\n",
    "#                             metrics.BinaryIoU(),\n",
    "#                             'accuracy'])\n",
    "model.compile(optimizer=optimizer1,\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=[metrics.Precision(),\n",
    "                            metrics.Recall(),\n",
    "                            metrics.FalsePositives(),\n",
    "                            metrics.FalseNegatives(),\n",
    "                            metrics.BinaryIoU(),\n",
    "                            'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "# Paths to your data.\n",
    "images_path = './archive/crack_segmentation_dataset/train/images/'\n",
    "masks_path = './archive/crack_segmentation_dataset/train/masks/'\n",
    "img_size = (112, 112)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(images_path, masks_path, img_size):\n",
    "    images = []  # List to store the images\n",
    "    masks = []   # List to store the masks\n",
    "    path = ''\n",
    "    count = 0\n",
    "    for filename in os.listdir(images_path):\n",
    "        if count > 500:\n",
    "            break\n",
    "        img_path = images_path + filename\n",
    "        mask_path = masks_path + filename  # Assuming mask has same filename\n",
    "        # print(img_path)\n",
    "        # Load and preprocess the image\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # print(img.size())\n",
    "    #   if not img or not mask:\n",
    "    #         continue\n",
    "        img = cv2.resize(img, img_size) / 255.0\n",
    "\n",
    "        # Load and preprocess the mask using OpenCV\n",
    "        \n",
    "        mask = cv2.resize(mask, img_size) / 255.0\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "        count += 1\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "images, masks = load_data(images_path, masks_path, (112, 112))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 112, 112, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, masks1 = images[:500], masks[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks1 = np.expand_dims(masks1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 112, 112, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (400, 112, 112, 3)\n",
      "y_train shape: (400, 112, 112, 1)\n",
      "X_val shape: (100, 112, 112, 3)\n",
      "y_val shape: (100, 112, 112, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(images1, masks1, test_size=0.2)\n",
    "X_train = X_train.reshape((-1, 112, 112, 3))\n",
    "X_val = X_val.reshape((-1, 112, 112, 3))\n",
    "y_train = y_train.reshape((-1, 112, 112, 1))\n",
    "y_val = y_val.reshape((-1, 112, 112, 1))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 8s/step - accuracy: 0.9095 - binary_io_u: 0.4728 - false_negatives: 147329.8438 - false_positives: 60905.0000 - loss: 0.2839 - precision: 0.0215 - recall: 0.0159 - val_accuracy: 0.9471 - val_binary_io_u: 0.4910 - val_false_negatives: 63709.0000 - val_false_positives: 0.0000e+00 - val_loss: 2692529520640.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.9550 - binary_io_u: 0.4913 - false_negatives: 9027.0000 - false_positives: 0.0000e+00 - loss: 0.1119 - precision: 0.0000e+00 - recall: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.9550 - binary_io_u: 0.4913 - false_negatives: 9027.0000 - false_positives: 0.0000e+00 - loss: 0.0606 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9549 - val_binary_io_u: 0.4956 - val_false_negatives: 2265.0000 - val_false_positives: 0.0000e+00 - val_loss: 330331881472.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9s/step - accuracy: 0.9518 - binary_io_u: 0.4916 - false_negatives: 127642.4609 - false_positives: 0.0000e+00 - loss: 0.1009 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9463 - val_binary_io_u: 0.4909 - val_false_negatives: 64668.0000 - val_false_positives: 0.0000e+00 - val_loss: 317800864.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9347 - binary_io_u: 0.4885 - false_negatives: 26220.0000 - false_positives: 0.0000e+00 - loss: 0.0601 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9740 - val_binary_io_u: 0.4979 - val_false_negatives: 1306.0000 - val_false_positives: 0.0000e+00 - val_loss: 37065832.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 0.9503 - binary_io_u: 0.4914 - false_negatives: 131486.0781 - false_positives: 0.0000e+00 - loss: 0.0935 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9465 - val_binary_io_u: 0.4910 - val_false_negatives: 64451.0000 - val_false_positives: 0.0000e+00 - val_loss: 3057989.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9544 - binary_io_u: 0.4916 - false_negatives: 18301.0000 - false_positives: 0.0000e+00 - loss: 0.0447 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9696 - val_binary_io_u: 0.4962 - val_false_negatives: 1523.0000 - val_false_positives: 0.0000e+00 - val_loss: 389423.3438 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10s/step - accuracy: 0.9485 - binary_io_u: 0.4911 - false_negatives: 143211.3906 - false_positives: 0.0000e+00 - loss: 0.0887 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9478 - val_binary_io_u: 0.4913 - val_false_negatives: 62917.0000 - val_false_positives: 0.0000e+00 - val_loss: 23997.7422 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9514 - binary_io_u: 0.4917 - false_negatives: 9757.0000 - false_positives: 0.0000e+00 - loss: 0.0429 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9391 - val_binary_io_u: 0.4890 - val_false_negatives: 3057.0000 - val_false_positives: 0.0000e+00 - val_loss: 10805.8877 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 10s/step - accuracy: 0.9491 - binary_io_u: 0.4912 - false_negatives: 132491.6875 - false_positives: 0.0000e+00 - loss: 0.0862 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9474 - val_binary_io_u: 0.4911 - val_false_negatives: 63345.0000 - val_false_positives: 0.0000e+00 - val_loss: 10098.6689 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.9472 - binary_io_u: 0.4906 - false_negatives: 21181.0000 - false_positives: 0.0000e+00 - loss: 0.0544 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9476 - val_binary_io_u: 0.4934 - val_false_negatives: 2629.0000 - val_false_positives: 0.0000e+00 - val_loss: 4950.8550 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data generators with desired augmentation parameters\n",
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "\n",
    "# Generate batches of augmented data from arrays (X_train and y_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)\n",
    "\n",
    "# Train the model using the fit_generator function\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=len(X_train) // 32,  # Number of batches per epoch\n",
    "    epochs=10, \n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // 32  # Number of batches for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 112, 112, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = 'CFD_001.jpg'\n",
    "input_image = cv2.imread(image_path)\n",
    "cv2.imwrite( \"test_input.jpg\", input_image)\n",
    "input_image = cv2.resize(input_image, (112, 112))/255.0  # Resize to match your model's input shape\n",
    "# image = image / 255.0  # Normalize the pixel values\n",
    "\n",
    "input_image = np.expand_dims(input_image, axis=0) \n",
    "# Convert the image to a NumPy array with the expected shape\n",
    "# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 112, 112, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict([input_image])\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE90lEQVR4nO3bIQ7EMAwAweTU/3/ZxxaHRC2YwQZmKwPvmZkFAGut39sLAPAdogBARAGAiAIAEQUAIgoARBQAiCgAkOd0cO99cw8ALjv5VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ53RwZm7uAcAHuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgfGXENBwN97qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_image = prediction[0]\n",
    "\n",
    "# Now, display the image\n",
    "plt.imshow(prediction_image, cmap='gray')  # Use an appropriate colormap if not binary\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
