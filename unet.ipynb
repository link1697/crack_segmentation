{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = layers.concatenate([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 1024)\n",
    "    # b1 = conv_block(p4, 2048)\n",
    "\n",
    "    # Decoder\n",
    "    # d1 = decoder_block(b1, s4, 1024)\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (img_size, img_size, 3)  # Adjust size and channels according to your dataset\n",
    "model = build_unet(input_shape)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./archive/crack_segmentation_dataset/train/images/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "# Paths to your data.\n",
    "images_path = './archive/crack_segmentation_dataset/train/images/'\n",
    "masks_path = './archive/crack_segmentation_dataset/train/masks/'\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(images_path, masks_path, img_size):\n",
    "    images = []  # List to store the images\n",
    "    masks = []   # List to store the masks\n",
    "    path = ''\n",
    "    count = 0\n",
    "    print(images_path)\n",
    "    for filename in os.listdir(images_path):\n",
    "        # if count > 500:\n",
    "        #     break\n",
    "        img_path = images_path + filename\n",
    "        mask_path = masks_path + filename  # Assuming mask has same filename\n",
    "        # print(img_path, \"Sagrfgh\")\n",
    "        # print(img_path)\n",
    "        # Load and preprocess the image\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # print(img.size())\n",
    "    #   if not img or not mask:\n",
    "    #         continue\n",
    "        img = cv2.resize(img, img_size) / 255.0\n",
    "\n",
    "        # Load and preprocess the mask using OpenCV\n",
    "        \n",
    "        mask = cv2.resize(mask, img_size) / 255.0\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "        count += 1\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n",
    "images, masks = load_data(images_path, masks_path, (img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9603, 224, 224, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, masks1 = images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks1 = np.expand_dims(masks1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9603, 224, 224, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7682, 224, 224, 3)\n",
      "y_train shape: (7682, 224, 224, 1)\n",
      "X_val shape: (1921, 224, 224, 3)\n",
      "y_val shape: (1921, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(images1, masks1, test_size=0.2)\n",
    "X_train = X_train.reshape((-1, img_size, img_size, 3))\n",
    "X_val = X_val.reshape((-1, img_size, img_size, 3))\n",
    "y_train = y_train.reshape((-1, img_size, img_size, 1))\n",
    "y_val = y_val.reshape((-1, img_size, img_size, 1))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 34/240\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09:36\u001b[0m 38s/step - accuracy: 0.9180 - loss: 0.2939"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data generators with desired augmentation parameters\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "\n",
    "# Generate batches of augmented data from arrays (X_train and y_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)\n",
    "\n",
    "# Train the model using the fit_generator function\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=len(X_train) // 32,  # Number of batches per epoch\n",
    "    epochs=10, \n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val) // 32  # Number of batches for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masg.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, image)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the image to a NumPy array with the expected shape\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43minput_image\u001b[49m)\n\u001b[0;32m     10\u001b[0m prediction\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_image' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = 'CFD_001.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.resize(image, (112, 112))  # Resize to match your model's input shape\n",
    "# image = image / 255.0  # Normalize the pixel values\n",
    "cv2.imwrite( \"asg.jpg\", image)\n",
    "# Convert the image to a NumPy array with the expected shape\n",
    "# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
    "prediction = model.predict(input_image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mprediction\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      6\u001b[0m prediction_2d \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction *= 255\n",
    "\n",
    "prediction_2d = prediction.squeeze()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(prediction_2d, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def gaussianNoise(image):\n",
    "    row, col, ch = image.shape\n",
    "    mean = 0\n",
    "    var = 0.5\n",
    "    sigma = var ** 0.5\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "    gauss = gauss.reshape(row, col, ch).astype('uint8')\n",
    "    noisy_image = cv2.add(image, gauss)\n",
    "    return noisy_image\n",
    "def gaussianBlur(image, kernel_size=(5, 5)):\n",
    "    blurred_image = cv2.GaussianBlur(image, kernel_size, 0)\n",
    "    return blurred_image\n",
    "def SobelFilter(image):\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    sobel = cv2.sqrt(cv2.addWeighted(cv2.pow(sobelx, 2), 0.5, cv2.pow(sobely, 2), 0.5, 0))\n",
    "    sobel = np.uint8(sobel)\n",
    "    noisy_sobel = gaussianNoise(sobel)\n",
    "    return noisy_sobel\n",
    "\n",
    "def colorDistortion(image, brightness=1, contrast=1):\n",
    "    # Clip the brightness and contrast values to prevent overflows\n",
    "    brightness = np.clip(brightness, -100, 100)\n",
    "    contrast = np.clip(contrast, -100, 100)\n",
    "\n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            highlight = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            highlight = 255 + brightness\n",
    "        alpha_b = (highlight - shadow) / 255\n",
    "        gamma_b = shadow\n",
    "        \n",
    "        buf = cv2.addWeighted(image, alpha_b, image, 0, gamma_b)\n",
    "    else:\n",
    "        buf = image.copy()\n",
    "\n",
    "    if contrast != 0:\n",
    "        f = 131 * (contrast + 127) / (127 * (131 - contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127 * (1 - f)\n",
    "        \n",
    "        buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n",
    "\n",
    "    return buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = 'CFD_001.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "# image = cv2.resize(image, (112, 112))  # Resize to match your model's input shape\n",
    "# image = image / 255.0  # Normalize the pixel values\n",
    "cv2.imwrite( \"GaussianNoise.jpg\", gaussianNoise(image))\n",
    "cv2.imwrite( \"GaussianBlur.jpg\", gaussianBlur(image))\n",
    "cv2.imwrite( \"SobelFilter.jpg\", SobelFilter(image))\n",
    "cv2.imwrite( \"colorDistortion.jpg\", colorDistortion(image))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inputs have incompatible shapes. Received shapes (128, 128, 64) and (128, 128, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Define your input shape\u001b[39;00m\n\u001b[0;32m     81\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Adjust size and channels according to your dataset\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m, in \u001b[0;36mbuild_unet\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(input_shape)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m s1, p1 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m s2, p2 \u001b[38;5;241m=\u001b[39m encoder_block(p1, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     62\u001b[0m s3, p3 \u001b[38;5;241m=\u001b[39m encoder_block(p2, \u001b[38;5;241m256\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m, in \u001b[0;36mencoder_block\u001b[1;34m(input_tensor, num_filters)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_block\u001b[39m(input_tensor, num_filters):\n\u001b[1;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     p \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))(x)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, p\n",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m, in \u001b[0;36mconv_block\u001b[1;34m(input_tensor, num_filters)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv_block\u001b[39m(input_tensor, num_filters):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Modify the conv_block to include a residual structure\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mresidual_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m, in \u001b[0;36mresidual_block\u001b[1;34m(input_tensor, num_filters)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mBatchNormalization()(x)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Add the input_tensor (residual connection) to the output of the convolution block\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mReLU()(x)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\merging\\add.py:69\u001b[0m, in \u001b[0;36madd\u001b[1;34m(inputs, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.add\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `keras.layers.Add` layer.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAdd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\link\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\merging\\base_merge.py:56\u001b[0m, in \u001b[0;36mMerge._compute_elemwise_op_output_shape\u001b[1;34m(self, shape1, shape2)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m---> 56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     57\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs have incompatible shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m             )\n\u001b[0;32m     60\u001b[0m         output_shape\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(output_shape)\n",
      "\u001b[1;31mValueError\u001b[0m: Inputs have incompatible shapes. Received shapes (128, 128, 64) and (128, 128, 3)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Residual block\n",
    "def residual_block(input_tensor, num_filters):\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add the input_tensor (residual connection) to the output of the convolution block\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Attention gate\n",
    "def attention_gate(input_tensor, gate_tensor, num_filters):\n",
    "    # Reshape gate tensor to match the input_tensor shape\n",
    "    gate_resized = layers.Conv2D(num_filters, 1, padding='same')(gate_tensor)\n",
    "    gate_resized = layers.BatchNormalization()(gate_resized)\n",
    "\n",
    "    # Add the gate to the input_tensor\n",
    "    x = layers.add([input_tensor, gate_resized])\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    # Sigmoid activation to get the attention coefficients\n",
    "    attention = layers.Conv2D(1, 1, padding='same', activation='sigmoid')(x)\n",
    "    x = layers.multiply([input_tensor, attention])\n",
    "\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    # Modify the conv_block to include a residual structure\n",
    "    x = residual_block(input_tensor, num_filters)\n",
    "    return x\n",
    "\n",
    "# Modify the encoder_block to return the residual block output\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    x = conv_block(input_tensor, num_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "# Modify the decoder_block to include an attention gate\n",
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_tensor)\n",
    "\n",
    "    # Apply attention gate to the skip features before concatenating\n",
    "    attention_skipped = attention_gate(skip_features, x, num_filters)\n",
    "    \n",
    "    x = layers.concatenate([x, attention_skipped])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Define your input shape\n",
    "input_shape = (128, 128, 3)  # Adjust size and channels according to your dataset\n",
    "model = build_unet(input_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
