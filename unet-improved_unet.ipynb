{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras import losses\n",
    " \n",
    "import random\n",
    "from keras import metrics\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Residual block\n",
    "# def residual_block(input_tensor, num_filters):\n",
    "#     x = layers.Conv2D(num_filters, 3, padding=\"same\")(input_tensor)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "\n",
    "#     x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "\n",
    "#     # Add the input_tensor (residual connection) to the output of the convolution block\n",
    "#     x = layers.add([x, input_tensor])\n",
    "#     x = layers.ReLU()(x)\n",
    "\n",
    "#     return x\n",
    "def residual_block(input_tensor, num_filters):\n",
    "    # Check if a projection shortcut is needed\n",
    "    input_shape = input_tensor.shape[-1]\n",
    "    if input_shape != num_filters:\n",
    "        shortcut = layers.Conv2D(num_filters, (1, 1), strides=(1, 1), padding='same')(input_tensor)\n",
    "    else:\n",
    "        shortcut = input_tensor\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add the shortcut (which has been adjusted if necessary) to the output of the convolution block\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "# Attention gate\n",
    "def attention_gate(input_tensor, gate_tensor, num_filters):\n",
    "    # Resize gate tensor to match the input_tensor shape using a 1x1 convolution\n",
    "    gate_resized = layers.Conv2D(num_filters, 1, padding='same')(gate_tensor)\n",
    "    gate_resized = layers.BatchNormalization()(gate_resized)\n",
    "    gate_resized = layers.ReLU()(gate_resized)\n",
    "\n",
    "    # Add the gate to the input_tensor\n",
    "    x = layers.add([input_tensor, gate_resized])\n",
    "    attention = layers.Conv2D(num_filters, 1, padding='same', activation='sigmoid')(x)\n",
    "    x = layers.multiply([input_tensor, attention])\n",
    "\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    x = residual_block(input_tensor, num_filters)\n",
    "    return x\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    x = conv_block(input_tensor, num_filters)\n",
    "    p = layers.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "# Modify the decoder_block to include an attention gate\n",
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_tensor)\n",
    "\n",
    "    # Apply attention gate to the skip features before concatenating\n",
    "    attention_skipped = attention_gate(skip_features, x, num_filters)\n",
    "\n",
    "    # Concatenate upsampled input with the attention modified skip features\n",
    "    x = layers.concatenate([x, attention_skipped])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (112, 112, 3)\n",
    "model = build_unet(input_shape)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=[metrics.Precision(),\n",
    "                            metrics.Recall(),\n",
    "                            metrics.FalsePositives(),\n",
    "                            metrics.FalseNegatives(),\n",
    "                            metrics.BinaryIoU()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to your data.\n",
    "images_path = './archive/crack_segmentation_dataset/train/images/'\n",
    "masks_path = './archive/crack_segmentation_dataset/train/masks/'\n",
    "img_size = (112, 112)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(images_path, masks_path, img_size):\n",
    "    images = []  # List to store the images\n",
    "    masks = []   # List to store the masks\n",
    "    path = ''\n",
    "    files = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(images_path):\n",
    "        if count > 500:\n",
    "            break\n",
    "        img_path = images_path + filename\n",
    "        mask_path = masks_path + filename  # Assuming mask has same filename\n",
    "        # print(img_path)\n",
    "        # Load and preprocess the image\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # print(img.size())\n",
    "    #   if not img or not mask:\n",
    "    #         continue\n",
    "        img, mask = img/255.0, mask/255.0\n",
    "        # img = cv2.resize(img, img_size) / 255.0\n",
    "        # # Load and preprocess the mask using OpenCV\n",
    "        # mask = cv2.resize(mask, img_size) / 255.0\n",
    " \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                img_part = img[i * 112: (i + 1) * 112,\n",
    "                            j * 112: (j + 1) * 112]\n",
    "                mask_part = mask[i * 112: (i + 1) * 112,\n",
    "                            j * 112: (j + 1) * 112]\n",
    "                images.append(img_part)\n",
    "                masks.append(mask_part)\n",
    "        files.append(filename)\n",
    "         \n",
    "        count += 1\n",
    "    print(files)\n",
    "    return np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(images_path, masks_path, img_size):\n",
    "    images = []  # List to store the images\n",
    "    masks = []   # List to store the masks\n",
    "    path = ''\n",
    "    count = 0\n",
    "    print()\n",
    "    imgList = os.listdir(images_path)\n",
    "    files = []\n",
    "    for i in range(len(imgList) - 1, -1, -1):\n",
    "        filename = imgList[i]\n",
    "    # for filename in os.listdir(images_path):\n",
    "        if count >= 100:\n",
    "            break\n",
    "\n",
    "        img_path = images_path + filename\n",
    "        mask_path = masks_path + filename  # Assuming mask has same filename\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img, mask = img/255.0, mask/255.0\n",
    "        while 1:\n",
    "            i, j = random.randint(0, 3), random.randint(0, 3)\n",
    "            img_part = img[i * 112: (i + 1) * 112,\n",
    "                        j * 112: (j + 1) * 112]\n",
    "            mask_part = mask[i * 112: (i + 1) * 112,\n",
    "                        j * 112: (j + 1) * 112]\n",
    "            if np.sum(mask_part) > 0:\n",
    "                break\n",
    "        images.append(img_part)\n",
    "        masks.append(mask_part)\n",
    "\n",
    "        files.append(filename)\n",
    "        count += 1\n",
    "    print(files)\n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CFD_002.jpg', 'CFD_003.jpg', 'CFD_004.jpg', 'CFD_005.jpg', 'CFD_006.jpg', 'CFD_008.jpg', 'CFD_009.jpg', 'CFD_010.jpg', 'CFD_012.jpg', 'CFD_015.jpg', 'CFD_016.jpg', 'CFD_017.jpg', 'CFD_018.jpg', 'CFD_020.jpg', 'CFD_021.jpg', 'CFD_022.jpg', 'CFD_023.jpg', 'CFD_024.jpg', 'CFD_025.jpg', 'CFD_026.jpg', 'CFD_027.jpg', 'CFD_028.jpg', 'CFD_029.jpg', 'CFD_030.jpg', 'CFD_031.jpg', 'CFD_032.jpg', 'CFD_033.jpg', 'CFD_034.jpg', 'CFD_035.jpg', 'CFD_036.jpg', 'CFD_038.jpg', 'CFD_039.jpg', 'CFD_041.jpg', 'CFD_042.jpg', 'CFD_043.jpg', 'CFD_044.jpg', 'CFD_045.jpg', 'CFD_046.jpg', 'CFD_048.jpg', 'CFD_049.jpg', 'CFD_050.jpg', 'CFD_051.jpg', 'CFD_052.jpg', 'CFD_053.jpg', 'CFD_054.jpg', 'CFD_055.jpg', 'CFD_056.jpg', 'CFD_057.jpg', 'CFD_058.jpg', 'CFD_059.jpg', 'CFD_060.jpg', 'CFD_061.jpg', 'CFD_062.jpg', 'CFD_063.jpg', 'CFD_064.jpg', 'CFD_065.jpg', 'CFD_066.jpg', 'CFD_067.jpg', 'CFD_068.jpg', 'CFD_069.jpg', 'CFD_071.jpg', 'CFD_072.jpg', 'CFD_073.jpg', 'CFD_074.jpg', 'CFD_075.jpg', 'CFD_076.jpg', 'CFD_077.jpg', 'CFD_078.jpg', 'CFD_079.jpg', 'CFD_081.jpg', 'CFD_083.jpg', 'CFD_085.jpg', 'CFD_086.jpg', 'CFD_087.jpg', 'CFD_089.jpg', 'CFD_090.jpg', 'CFD_091.jpg', 'CFD_092.jpg', 'CFD_093.jpg', 'CFD_094.jpg', 'CFD_095.jpg', 'CFD_096.jpg', 'CFD_097.jpg', 'CFD_098.jpg', 'CFD_099.jpg', 'CFD_100.jpg', 'CFD_101.jpg', 'CFD_102.jpg', 'CFD_103.jpg', 'CFD_104.jpg', 'CFD_105.jpg', 'CFD_106.jpg', 'CFD_107.jpg', 'CFD_110.jpg', 'CFD_112.jpg', 'CFD_113.jpg', 'CFD_114.jpg', 'CFD_115.jpg', 'CFD_116.jpg', 'CFD_117.jpg', 'CRACK500_20160222_080850_1281_361.jpg', 'CRACK500_20160222_080850_1281_721.jpg', 'CRACK500_20160222_080850_1921_1.jpg', 'CRACK500_20160222_080850_1921_361.jpg', 'CRACK500_20160222_080850_1_361.jpg', 'CRACK500_20160222_080850_641_361.jpg', 'CRACK500_20160222_080850_641_721.jpg', 'CRACK500_20160222_080933_361_1281.jpg', 'CRACK500_20160222_080933_361_1921.jpg', 'CRACK500_20160222_080933_361_641.jpg', 'CRACK500_20160222_080933_721_1.jpg', 'CRACK500_20160222_080933_721_1921.jpg', 'CRACK500_20160222_080933_721_641.jpg', 'CRACK500_20160222_081011_1281_721.jpg', 'CRACK500_20160222_081011_1921_721.jpg', 'CRACK500_20160222_081011_1_361.jpg', 'CRACK500_20160222_081011_1_721.jpg', 'CRACK500_20160222_081011_641_361.jpg', 'CRACK500_20160222_081011_641_721.jpg', 'CRACK500_20160222_081031_1281_361.jpg', 'CRACK500_20160222_081031_1921_361.jpg', 'CRACK500_20160222_081031_1921_721.jpg', 'CRACK500_20160222_081031_1_721.jpg', 'CRACK500_20160222_081031_641_721.jpg', 'CRACK500_20160222_081102_1281_361.jpg', 'CRACK500_20160222_081102_1281_721.jpg', 'CRACK500_20160222_081102_1921_1.jpg', 'CRACK500_20160222_081102_1921_1081.jpg', 'CRACK500_20160222_081102_1921_361.jpg', 'CRACK500_20160222_081102_641_361.jpg', 'CRACK500_20160222_081102_641_721.jpg', 'CRACK500_20160222_081111_1281_361.jpg', 'CRACK500_20160222_081111_1281_721.jpg', 'CRACK500_20160222_081111_1921_721.jpg', 'CRACK500_20160222_081111_1_1.jpg', 'CRACK500_20160222_081111_1_361.jpg', 'CRACK500_20160222_081113_1281_361.jpg', 'CRACK500_20160222_081113_1281_721.jpg', 'CRACK500_20160222_081113_1_1.jpg', 'CRACK500_20160222_081113_1_361.jpg', 'CRACK500_20160222_081113_1_721.jpg', 'CRACK500_20160222_081113_641_361.jpg', 'CRACK500_20160222_081113_641_721.jpg', 'CRACK500_20160222_081839_1281_1.jpg', 'CRACK500_20160222_081839_1281_1081.jpg', 'CRACK500_20160222_081839_1281_361.jpg', 'CRACK500_20160222_081839_1281_721.jpg', 'CRACK500_20160222_081839_1921_361.jpg', 'CRACK500_20160222_081839_641_721.jpg', 'CRACK500_20160222_081908_1281_1.jpg', 'CRACK500_20160222_081908_1281_1081.jpg', 'CRACK500_20160222_081908_1281_361.jpg', 'CRACK500_20160222_081908_1281_721.jpg', 'CRACK500_20160222_081908_1_1.jpg', 'CRACK500_20160222_081908_1_1081.jpg', 'CRACK500_20160222_081908_1_361.jpg', 'CRACK500_20160222_081908_1_721.jpg', 'CRACK500_20160222_081908_641_361.jpg', 'CRACK500_20160222_082414_1281_1081.jpg', 'CRACK500_20160222_082414_1281_361.jpg', 'CRACK500_20160222_082414_1281_721.jpg', 'CRACK500_20160222_082414_1_361.jpg', 'CRACK500_20160222_082414_641_361.jpg', 'CRACK500_20160222_114759_1281_1.jpg', 'CRACK500_20160222_114759_1281_1081.jpg', 'CRACK500_20160222_114759_1281_361.jpg', 'CRACK500_20160222_114759_1281_721.jpg', 'CRACK500_20160222_114759_1921_1.jpg', 'CRACK500_20160222_114759_1921_1081.jpg', 'CRACK500_20160222_114759_1_1081.jpg', 'CRACK500_20160222_114759_1_361.jpg', 'CRACK500_20160222_114759_1_721.jpg', 'CRACK500_20160222_114759_641_1.jpg', 'CRACK500_20160222_114759_641_1081.jpg', 'CRACK500_20160222_114759_641_721.jpg', 'CRACK500_20160222_114806_1281_1.jpg', 'CRACK500_20160222_114806_1281_1081.jpg', 'CRACK500_20160222_114806_1281_361.jpg', 'CRACK500_20160222_114806_1281_721.jpg', 'CRACK500_20160222_115219_1281_361.jpg', 'CRACK500_20160222_115219_1921_1081.jpg', 'CRACK500_20160222_115219_641_1.jpg', 'CRACK500_20160222_115219_641_361.jpg', 'CRACK500_20160222_115224_1281_1.jpg', 'CRACK500_20160222_115224_1281_721.jpg', 'CRACK500_20160222_115224_1921_1081.jpg', 'CRACK500_20160222_115224_641_1.jpg', 'CRACK500_20160222_115224_641_1081.jpg', 'CRACK500_20160222_115224_641_361.jpg', 'CRACK500_20160222_115224_641_721.jpg', 'CRACK500_20160222_115233_1281_361.jpg', 'CRACK500_20160222_115233_1281_721.jpg', 'CRACK500_20160222_115233_1921_721.jpg', 'CRACK500_20160222_115233_1_361.jpg', 'CRACK500_20160222_115233_1_721.jpg', 'CRACK500_20160222_115233_641_361.jpg', 'CRACK500_20160222_115305_1921_1081.jpg', 'CRACK500_20160222_115305_641_1081.jpg', 'CRACK500_20160222_115305_641_361.jpg', 'CRACK500_20160222_115324_1281_1.jpg', 'CRACK500_20160222_115324_1281_361.jpg', 'CRACK500_20160222_115324_1_1.jpg', 'CRACK500_20160222_115324_641_361.jpg', 'CRACK500_20160222_115324_641_721.jpg', 'CRACK500_20160222_115519_641_1.jpg', 'CRACK500_20160222_115519_641_1081.jpg', 'CRACK500_20160222_115519_641_361.jpg', 'CRACK500_20160222_115620_1281_1.jpg', 'CRACK500_20160222_115620_641_1081.jpg', 'CRACK500_20160222_115620_641_721.jpg', 'CRACK500_20160222_115714_1281_1081.jpg', 'CRACK500_20160222_115714_1281_721.jpg', 'CRACK500_20160222_115714_641_1.jpg', 'CRACK500_20160222_115714_641_361.jpg', 'CRACK500_20160222_115738_1281_1.jpg', 'CRACK500_20160222_115738_1281_1081.jpg', 'CRACK500_20160222_115738_1281_361.jpg', 'CRACK500_20160222_115738_1281_721.jpg', 'CRACK500_20160222_115738_641_1081.jpg', 'CRACK500_20160222_115805_641_1.jpg', 'CRACK500_20160222_115805_641_1081.jpg', 'CRACK500_20160222_115805_641_361.jpg', 'CRACK500_20160222_115805_641_721.jpg', 'CRACK500_20160222_115828_641_1081.jpg', 'CRACK500_20160222_115828_641_361.jpg', 'CRACK500_20160222_115828_641_721.jpg', 'CRACK500_20160222_115833_1281_1081.jpg', 'CRACK500_20160222_115833_1281_361.jpg', 'CRACK500_20160222_115833_1281_721.jpg', 'CRACK500_20160222_115833_641_1.jpg', 'CRACK500_20160222_115837_1281_1.jpg', 'CRACK500_20160222_115837_1281_1081.jpg', 'CRACK500_20160222_115837_1281_361.jpg', 'CRACK500_20160222_115837_1281_721.jpg', 'CRACK500_20160222_115843_1281_1.jpg', 'CRACK500_20160222_115843_1281_721.jpg', 'CRACK500_20160222_115843_641_1081.jpg', 'CRACK500_20160222_115847_1281_1.jpg', 'CRACK500_20160222_115847_641_1081.jpg', 'CRACK500_20160222_115847_641_721.jpg', 'CRACK500_20160222_115858_641_1.jpg', 'CRACK500_20160222_115858_641_1081.jpg', 'CRACK500_20160222_115858_641_361.jpg', 'CRACK500_20160222_115858_641_721.jpg', 'CRACK500_20160222_163930_1281_361.jpg', 'CRACK500_20160222_163930_1281_721.jpg', 'CRACK500_20160222_163930_1921_1081.jpg', 'CRACK500_20160222_163930_1921_721.jpg', 'CRACK500_20160222_163930_1_1.jpg', 'CRACK500_20160222_163930_1_1081.jpg', 'CRACK500_20160222_163930_1_361.jpg', 'CRACK500_20160222_163930_1_721.jpg', 'CRACK500_20160222_163930_641_1.jpg', 'CRACK500_20160222_163930_641_361.jpg', 'CRACK500_20160222_163930_641_721.jpg', 'CRACK500_20160222_163940_1281_721.jpg', 'CRACK500_20160222_163940_1_1.jpg', 'CRACK500_20160222_163940_1_721.jpg', 'CRACK500_20160222_163940_641_1.jpg', 'CRACK500_20160222_163940_641_361.jpg', 'CRACK500_20160222_163940_641_721.jpg', 'CRACK500_20160222_164000_1281_1.jpg', 'CRACK500_20160222_164000_1281_1081.jpg', 'CRACK500_20160222_164000_1281_361.jpg', 'CRACK500_20160222_164000_1281_721.jpg', 'CRACK500_20160222_164000_1921_1.jpg', 'CRACK500_20160222_164000_1921_361.jpg', 'CRACK500_20160222_164000_1_1.jpg', 'CRACK500_20160222_164000_1_361.jpg', 'CRACK500_20160222_164000_1_721.jpg', 'CRACK500_20160222_164000_641_1.jpg', 'CRACK500_20160222_164000_641_1081.jpg', 'CRACK500_20160222_164000_641_361.jpg', 'CRACK500_20160222_164000_641_721.jpg', 'CRACK500_20160222_164021_1281_1.jpg', 'CRACK500_20160222_164021_1281_361.jpg', 'CRACK500_20160222_164021_641_1081.jpg', 'CRACK500_20160222_164021_641_361.jpg', 'CRACK500_20160222_164021_641_721.jpg', 'CRACK500_20160222_164141_1281_361.jpg', 'CRACK500_20160222_164141_1281_721.jpg', 'CRACK500_20160222_164141_1921_1.jpg', 'CRACK500_20160222_164141_1921_1081.jpg', 'CRACK500_20160222_164141_1921_361.jpg', 'CRACK500_20160222_164141_1_1.jpg', 'CRACK500_20160222_164141_1_361.jpg', 'CRACK500_20160222_164141_1_721.jpg', 'CRACK500_20160222_164141_641_1.jpg', 'CRACK500_20160222_164141_641_1081.jpg', 'CRACK500_20160222_164141_641_361.jpg', 'CRACK500_20160222_164141_641_721.jpg', 'CRACK500_20160222_164822_1281_361.jpg', 'CRACK500_20160222_164822_1281_721.jpg', 'CRACK500_20160222_164822_641_1081.jpg', 'CRACK500_20160222_164822_641_721.jpg', 'CRACK500_20160222_164825_1281_1.jpg', 'CRACK500_20160222_164825_1281_361.jpg', 'CRACK500_20160222_164825_1281_721.jpg', 'CRACK500_20160222_164825_641_1081.jpg', 'CRACK500_20160222_164851_1281_1081.jpg', 'CRACK500_20160222_164851_641_1.jpg', 'CRACK500_20160222_164851_641_361.jpg', 'CRACK500_20160222_164851_641_721.jpg', 'CRACK500_20160222_164920_641_1.jpg', 'CRACK500_20160222_164920_641_361.jpg', 'CRACK500_20160222_164920_641_721.jpg', 'CRACK500_20160222_164936_641_1.jpg', 'CRACK500_20160222_164936_641_1081.jpg', 'CRACK500_20160222_164936_641_721.jpg', 'CRACK500_20160222_165012_1281_1.jpg', 'CRACK500_20160222_165012_641_1.jpg', 'CRACK500_20160222_165012_641_361.jpg', 'CRACK500_20160222_165012_641_721.jpg', 'CRACK500_20160222_165031_641_1.jpg', 'CRACK500_20160222_165031_641_1081.jpg', 'CRACK500_20160222_165031_641_361.jpg', 'CRACK500_20160222_165031_641_721.jpg', 'CRACK500_20160222_165037_1281_721.jpg', 'CRACK500_20160222_165037_1921_721.jpg', 'CRACK500_20160222_165037_1_721.jpg', 'CRACK500_20160222_165037_641_721.jpg', 'CRACK500_20160222_165218_1281_721.jpg', 'CRACK500_20160222_165218_1921_1081.jpg', 'CRACK500_20160222_165218_1921_721.jpg', 'CRACK500_20160222_165218_1_361.jpg', 'CRACK500_20160222_165218_641_361.jpg', 'CRACK500_20160222_165218_641_721.jpg', 'CRACK500_20160222_165225_1281_361.jpg', 'CRACK500_20160222_165225_1921_721.jpg', 'CRACK500_20160222_165225_1_361.jpg', 'CRACK500_20160222_165225_641_361.jpg', 'CRACK500_20160222_165256_1281_361.jpg', 'CRACK500_20160222_165256_1921_1081.jpg', 'CRACK500_20160222_165256_641_721.jpg', 'CRACK500_20160222_165402_1281_1.jpg', 'CRACK500_20160222_165402_1281_361.jpg', 'CRACK500_20160222_165402_641_1081.jpg', 'CRACK500_20160222_165402_641_721.jpg', 'CRACK500_20160222_165625_1281_361.jpg', 'CRACK500_20160222_165625_1921_361.jpg', 'CRACK500_20160222_165625_1921_721.jpg', 'CRACK500_20160222_165625_1_721.jpg', 'CRACK500_20160222_165625_641_361.jpg', 'CRACK500_20160222_165625_641_721.jpg', 'CRACK500_20160222_165715_1281_361.jpg', 'CRACK500_20160222_165715_1921_361.jpg', 'CRACK500_20160222_165715_1_721.jpg', 'CRACK500_20160222_165715_641_721.jpg', 'CRACK500_20160222_165909_1281_361.jpg', 'CRACK500_20160222_165909_1281_721.jpg', 'CRACK500_20160222_165909_1921_361.jpg', 'CRACK500_20160222_165909_1_1.jpg', 'CRACK500_20160222_165909_1_361.jpg', 'CRACK500_20160222_165909_641_361.jpg', 'CRACK500_20160222_165909_641_721.jpg', 'CRACK500_20160222_165919_1281_1.jpg', 'CRACK500_20160222_165919_1281_361.jpg', 'CRACK500_20160222_165919_1281_721.jpg', 'CRACK500_20160222_165919_1921_721.jpg', 'CRACK500_20160222_165919_1_361.jpg', 'CRACK500_20160222_165919_641_361.jpg', 'CRACK500_20160222_165919_641_721.jpg', 'CRACK500_20160222_165926_1281_361.jpg', 'CRACK500_20160222_165926_1921_721.jpg', 'CRACK500_20160222_165926_1_1081.jpg', 'CRACK500_20160222_165926_1_721.jpg', 'CRACK500_20160222_165926_641_1.jpg', 'CRACK500_20160222_165926_641_361.jpg', 'CRACK500_20160222_165926_641_721.jpg', 'CRACK500_20160222_165937_1281_361.jpg', 'CRACK500_20160222_165937_1281_721.jpg', 'CRACK500_20160222_165937_1921_721.jpg', 'CRACK500_20160222_165937_1_1081.jpg', 'CRACK500_20160222_165937_1_361.jpg', 'CRACK500_20160222_165937_1_721.jpg', 'CRACK500_20160222_165937_641_361.jpg', 'CRACK500_20160222_165937_641_721.jpg', 'CRACK500_20160222_165947_1281_721.jpg', 'CRACK500_20160222_165947_1_721.jpg', 'CRACK500_20160222_165947_641_1.jpg', 'CRACK500_20160222_165947_641_361.jpg', 'CRACK500_20160222_165947_641_721.jpg', 'CRACK500_20160222_165951_1281_1.jpg', 'CRACK500_20160222_165951_1281_1081.jpg', 'CRACK500_20160222_165951_1281_361.jpg', 'CRACK500_20160222_165951_1281_721.jpg', 'CRACK500_20160222_165951_1921_1081.jpg', 'CRACK500_20160222_165951_1921_361.jpg', 'CRACK500_20160222_165951_1921_721.jpg', 'CRACK500_20160222_165951_1_1.jpg', 'CRACK500_20160222_165951_641_1.jpg', 'CRACK500_20160222_165951_641_1081.jpg', 'CRACK500_20160222_165951_641_361.jpg', 'CRACK500_20160225_114503_1281_1.jpg', 'CRACK500_20160225_114503_1281_361.jpg', 'CRACK500_20160225_114503_1281_721.jpg', 'CRACK500_20160225_114503_1921_1.jpg', 'CRACK500_20160225_114503_1921_361.jpg', 'CRACK500_20160225_114503_1_1081.jpg', 'CRACK500_20160225_114503_1_721.jpg', 'CRACK500_20160225_114503_641_1.jpg', 'CRACK500_20160225_114503_641_1081.jpg', 'CRACK500_20160225_114503_641_361.jpg', 'CRACK500_20160225_114503_641_721.jpg', 'CRACK500_20160225_114514_1081_1.jpg', 'CRACK500_20160225_114514_1081_1281.jpg', 'CRACK500_20160225_114514_1081_1921.jpg', 'CRACK500_20160225_114514_1081_641.jpg', 'CRACK500_20160225_114514_1_1281.jpg', 'CRACK500_20160225_114514_1_641.jpg', 'CRACK500_20160225_114514_361_1.jpg', 'CRACK500_20160225_114514_361_1281.jpg', 'CRACK500_20160225_114514_361_641.jpg', 'CRACK500_20160225_114514_721_1.jpg', 'CRACK500_20160225_114531_1281_1.jpg', 'CRACK500_20160225_114531_1281_1081.jpg', 'CRACK500_20160225_114531_1281_361.jpg', 'CRACK500_20160225_114531_1281_721.jpg', 'CRACK500_20160225_114531_1921_1081.jpg', 'CRACK500_20160225_114531_1921_721.jpg', 'CRACK500_20160225_114531_641_1.jpg', 'CRACK500_20160225_114531_641_1081.jpg', 'CRACK500_20160225_114531_641_361.jpg', 'CRACK500_20160225_114531_641_721.jpg', 'CRACK500_20160225_114535_1281_1.jpg', 'CRACK500_20160225_114535_1281_361.jpg', 'CRACK500_20160225_114535_1281_721.jpg', 'CRACK500_20160225_114535_1921_721.jpg', 'CRACK500_20160225_114535_641_1.jpg', 'CRACK500_20160225_114535_641_361.jpg', 'CRACK500_20160225_114535_641_721.jpg', 'CRACK500_20160302_155848_1281_721.jpg', 'CRACK500_20160302_155848_1921_1.jpg', 'CRACK500_20160302_155848_1_721.jpg', 'CRACK500_20160302_155857_1281_361.jpg', 'CRACK500_20160302_155857_1281_721.jpg', 'CRACK500_20160302_155857_641_721.jpg', 'CRACK500_20160302_155914_361_1281.jpg', 'CRACK500_20160302_155914_361_1921.jpg', 'CRACK500_20160302_155914_361_641.jpg', 'CRACK500_20160302_155914_721_641.jpg', 'CRACK500_20160302_155919_1281_361.jpg', 'CRACK500_20160302_155919_1921_361.jpg', 'CRACK500_20160302_155919_641_721.jpg', 'CRACK500_20160302_155925_1281_1.jpg', 'CRACK500_20160302_155925_1921_361.jpg', 'CRACK500_20160302_155925_641_721.jpg', 'CRACK500_20160302_160033_1281_721.jpg', 'CRACK500_20160302_160033_1921_361.jpg', 'CRACK500_20160302_160033_1921_721.jpg', 'CRACK500_20160302_160033_1_361.jpg', 'CRACK500_20160302_160033_641_721.jpg', 'CRACK500_20160303_091825_1281_361.jpg', 'CRACK500_20160303_091825_1921_361.jpg', 'CRACK500_20160303_091825_1921_721.jpg', 'CRACK500_20160303_091825_1_1.jpg', 'CRACK500_20160303_091825_641_1.jpg', 'CRACK500_20160303_091825_641_1081.jpg', 'CRACK500_20160303_091825_641_361.jpg', 'CRACK500_20160303_091825_641_721.jpg', 'CRACK500_20160303_091830_1_1921.jpg', 'CRACK500_20160303_091830_361_1921.jpg', 'CRACK500_20160303_091830_721_1.jpg', 'CRACK500_20160303_091830_721_1281.jpg', 'CRACK500_20160303_091830_721_641.jpg', 'CRACK500_20160303_091837_361_1.jpg', 'CRACK500_20160303_091837_361_1281.jpg', 'CRACK500_20160303_091837_361_641.jpg', 'CRACK500_20160303_091837_721_1.jpg', 'CRACK500_20160303_091837_721_1281.jpg', 'CRACK500_20160303_091837_721_1921.jpg', 'CRACK500_20160303_092627_1281_361.jpg', 'CRACK500_20160303_092627_1921_361.jpg', 'CRACK500_20160303_092627_1921_721.jpg', 'CRACK500_20160303_092627_1_361.jpg', 'CRACK500_20160303_092627_1_721.jpg', 'CRACK500_20160303_092627_641_361.jpg', 'CRACK500_20160303_092806_1281_1.jpg', 'CRACK500_20160303_092806_1281_361.jpg', 'CRACK500_20160303_092806_1921_1081.jpg', 'CRACK500_20160303_092806_1921_721.jpg', 'CRACK500_20160303_092806_1_1081.jpg', 'CRACK500_20160303_092806_641_361.jpg', 'CRACK500_20160303_092806_641_721.jpg', 'CRACK500_20160303_092839_1921_1081.jpg', 'CRACK500_20160303_092839_1921_361.jpg', 'CRACK500_20160303_092839_1921_721.jpg', 'CRACK500_20160303_092839_1_721.jpg', 'CRACK500_20160303_093353_1281_361.jpg', 'CRACK500_20160303_093353_1281_721.jpg', 'CRACK500_20160303_093353_1_1081.jpg', 'CRACK500_20160303_093353_1_361.jpg', 'CRACK500_20160303_093353_1_721.jpg', 'CRACK500_20160303_093353_641_361.jpg', 'CRACK500_20160303_093400_1281_1081.jpg', 'CRACK500_20160303_093400_1281_361.jpg', 'CRACK500_20160303_093400_1281_721.jpg', 'CRACK500_20160303_093400_641_1081.jpg', 'CRACK500_20160303_093400_641_361.jpg', 'CRACK500_20160303_093527_1281_1.jpg', 'CRACK500_20160303_093527_1921_1.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "images, masks = load_data(images_path, masks_path, (112, 112))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8016, 112, 112, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, masks1 = images[:2000], masks[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Volker_DSC01710_725_197_1392_1586.jpg', 'Volker_DSC01710_608_6_1524_1817.jpg', 'Volker_DSC01710_521_437_1123_892.jpg', 'Volker_DSC01710_449_161_1824_1502.jpg', 'Volker_DSC01710_37_518_1605_1304.jpg', 'Volker_DSC01710_340_159_2049_1635.jpg', 'Volker_DSC01710_15_28_1388_1132.jpg', 'Volker_DSC01710_112_246_1993_1548.jpg', 'Volker_DSC01710_1087_22_1502_1742.jpg', 'Volker_DSC01710_1025_76_1293_1327.jpg', 'Volker_DSC01710_0_0_2736_1824.jpg', 'Volker_DSC01709_599_1337_1214_1373.jpg', 'Volker_DSC01709_525_1036_1256_1448.jpg', 'Volker_DSC01709_43_382_1337_1425.jpg', 'Volker_DSC01709_1_428_1791_1806.jpg', 'Volker_DSC01709_191_287_1561_2056.jpg', 'Volker_DSC01709_176_298_1435_1635.jpg', 'Volker_DSC01709_14_103_1566_1955.jpg', 'Volker_DSC01709_109_399_1680_2130.jpg', 'Volker_DSC01709_0_0_1824_2736.jpg', 'Volker_DSC01708_820_247_911_1098.jpg', 'Volker_DSC01708_7_339_1805_2017.jpg', 'Volker_DSC01708_6_308_1788_2155.jpg', 'Volker_DSC01708_3_1033_1692_1349.jpg', 'Volker_DSC01708_26_342_1730_1845.jpg', 'Volker_DSC01708_190_1369_1165_1016.jpg', 'Volker_DSC01708_162_428_1581_1891.jpg', 'Volker_DSC01708_14_25_1720_2189.jpg', 'Volker_DSC01708_146_565_1550_1877.jpg', 'Volker_DSC01708_0_0_1824_2736.jpg', 'Volker_DSC01707_70_122_1743_1638.jpg', 'Volker_DSC01707_631_1517_1144_1199.jpg', 'Volker_DSC01707_296_841_1466_1781.jpg', 'Volker_DSC01707_290_218_1225_1618.jpg', 'Volker_DSC01707_14_642_1208_1489.jpg', 'Volker_DSC01707_117_725_1538_1216.jpg', 'Volker_DSC01707_107_677_1669_2015.jpg', 'Volker_DSC01707_0_0_1824_2736.jpg', 'Volker_DSC01706_69_669_1577_1989.jpg', 'Volker_DSC01706_58_458_1678_1398.jpg', 'Volker_DSC01706_323_1714_1115_982.jpg', 'Volker_DSC01706_255_630_1554_1969.jpg', 'Volker_DSC01706_240_1123_1495_1323.jpg', 'Volker_DSC01706_183_735_1171_1164.jpg', 'Volker_DSC01706_131_863_1650_1722.jpg', 'Volker_DSC01706_121_1149_1463_1438.jpg', 'Volker_DSC01706_113_377_1590_1904.jpg', 'Volker_DSC01706_0_0_1824_2736.jpg', 'Volker_DSC01705_680_705_1083_950.jpg', 'Volker_DSC01705_446_137_1039_1159.jpg', 'Volker_DSC01705_389_1310_1098_920.jpg', 'Volker_DSC01705_221_527_1603_1965.jpg', 'Volker_DSC01705_181_365_1624_2023.jpg', 'Volker_DSC01705_108_1269_1575_1361.jpg', 'Volker_DSC01705_0_0_1824_2736.jpg', 'Volker_DSC01704_64_82_1215_1389.jpg', 'Volker_DSC01704_5_245_1818_2163.jpg', 'Volker_DSC01704_446_1296_1363_1130.jpg', 'Volker_DSC01704_365_495_1058_1256.jpg', 'Volker_DSC01704_35_556_1657_1753.jpg', 'Volker_DSC01704_229_314_1475_1939.jpg', 'Volker_DSC01704_226_39_928_1215.jpg', 'Volker_DSC01704_219_142_1156_1117.jpg', 'Volker_DSC01704_214_498_1312_1428.jpg', 'Volker_DSC01704_17_949_1389_1694.jpg', 'Volker_DSC01704_0_0_1824_2736.jpg', 'Volker_DSC01703_66_610_1432_1109.jpg', 'Volker_DSC01703_597_1216_1104_1108.jpg', 'Volker_DSC01703_574_1149_1207_1490.jpg', 'Volker_DSC01703_478_1348_1214_966.jpg', 'Volker_DSC01703_409_465_1334_1085.jpg', 'Volker_DSC01703_34_562_1772_1755.jpg', 'Volker_DSC01703_106_1066_1488_1583.jpg', 'Volker_DSC01703_0_0_1824_2736.jpg', 'Volker_DSC01702_80_579_1686_1357.jpg', 'Volker_DSC01702_53_496_1764_2075.jpg', 'Volker_DSC01702_485_615_1142_1023.jpg', 'Volker_DSC01702_353_632_1134_1498.jpg', 'Volker_DSC01702_293_97_1158_1277.jpg', 'Volker_DSC01702_14_320_1617_1798.jpg', 'Volker_DSC01702_148_426_1258_1569.jpg', 'Volker_DSC01702_128_622_1625_2108.jpg', 'Volker_DSC01702_0_0_1824_2736.jpg', 'Volker_DSC01701_84_515_1689_1933.jpg', 'Volker_DSC01701_676_1216_1050_1357.jpg', 'Volker_DSC01701_66_400_1209_1491.jpg', 'Volker_DSC01701_562_672_1127_1205.jpg', 'Volker_DSC01701_537_291_1230_1215.jpg', 'Volker_DSC01701_38_802_1782_1575.jpg', 'Volker_DSC01701_233_60_1569_1928.jpg', 'Volker_DSC01701_221_1280_1385_1324.jpg', 'Volker_DSC01701_212_870_1467_1795.jpg', 'Volker_DSC01701_0_0_1824_2736.jpg', 'Volker_DSC01700_82_197_1311_1539.jpg', 'Volker_DSC01700_403_348_1339_1574.jpg', 'Volker_DSC01700_3_1234_1751_1476.jpg', 'Volker_DSC01700_262_454_1451_1381.jpg', 'Volker_DSC01700_200_730_1473_1593.jpg', 'Volker_DSC01700_17_674_1805_1877.jpg', 'Volker_DSC01700_154_336_1381_1688.jpg']\n"
     ]
    }
   ],
   "source": [
    "masks1 = np.expand_dims(masks1, axis=-1)\n",
    "# Load your data\n",
    "imgval, maskval = load_test(images_path, masks_path, (112, 112))\n",
    "\n",
    "maskval = np.expand_dims(maskval, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 112, 112, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2000, 112, 112, 3)\n",
      "y_train shape: (2000, 112, 112, 1)\n",
      "X_val shape: (100, 112, 112, 3)\n",
      "y_val shape: (100, 112, 112, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train, X_val, y_val = images1, masks1,imgval, maskval \n",
    "# y_train.shape\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 7s/step - binary_io_u_1: 0.4598 - false_negatives_1: 137251.2344 - false_positives_1: 740278.8125 - loss: 0.8132 - precision_1: 0.1173 - recall_1: 0.4372 - val_binary_io_u_1: 0.0263 - val_false_negatives_1: 300.0000 - val_false_positives_1: 348478.0000 - val_loss: 0.8996 - val_precision_1: 0.0602 - val_recall_1: 0.9867\n",
      "Epoch 2/10\n",
      "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 7s/step - binary_io_u_1: 0.5671 - false_negatives_1: 144522.8438 - false_positives_1: 150618.1562 - loss: 0.6662 - precision_1: 0.3111 - recall_1: 0.3304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - binary_io_u_1: 0.5706 - false_negatives_1: 156849.4219 - false_positives_1: 157283.9531 - loss: 0.6545 - precision_1: 0.3210 - recall_1: 0.3336 - val_binary_io_u_1: 0.0139 - val_false_negatives_1: 3.0000 - val_false_positives_1: 357904.0000 - val_loss: 0.9203 - val_precision_1: 0.0487 - val_recall_1: 0.9998\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 7s/step - binary_io_u_1: 0.6231 - false_negatives_1: 146824.5938 - false_positives_1: 90186.9062 - loss: 0.5423 - precision_1: 0.4975 - recall_1: 0.3797 - val_binary_io_u_1: 0.1532 - val_false_negatives_1: 9709.0000 - val_false_positives_1: 253987.0000 - val_loss: 0.8794 - val_precision_1: 0.0773 - val_recall_1: 0.6866\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 7s/step - binary_io_u_1: 0.6290 - false_negatives_1: 154943.4219 - false_positives_1: 85514.1875 - loss: 0.5109 - precision_1: 0.5176 - recall_1: 0.3813 - val_binary_io_u_1: 0.1798 - val_false_negatives_1: 3033.0000 - val_false_positives_1: 78934.0000 - val_loss: 0.6125 - val_precision_1: 0.0515 - val_recall_1: 0.5858\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3699s\u001b[0m 184s/step - binary_io_u_1: 0.6411 - false_negatives_1: 142057.6406 - false_positives_1: 58422.1367 - loss: 0.4933 - precision_1: 0.5902 - recall_1: 0.3927 - val_binary_io_u_1: 0.4796 - val_false_negatives_1: 25757.0000 - val_false_positives_1: 3068.0000 - val_loss: 0.9798 - val_precision_1: 0.0936 - val_recall_1: 0.0122\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 11s/step - binary_io_u_1: 0.6917 - false_negatives_1: 147426.4688 - false_positives_1: 52777.6680 - loss: 0.3769 - precision_1: 0.7076 - recall_1: 0.4389 - val_binary_io_u_1: 0.4755 - val_false_negatives_1: 30774.0000 - val_false_positives_1: 0.0000e+00 - val_loss: 0.9995 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 13s/step - binary_io_u_1: 0.6860 - false_negatives_1: 135830.6875 - false_positives_1: 41100.7266 - loss: 0.3985 - precision_1: 0.7253 - recall_1: 0.4329 - val_binary_io_u_1: 0.4801 - val_false_negatives_1: 25477.0000 - val_false_positives_1: 0.0000e+00 - val_loss: 0.9996 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 13s/step - binary_io_u_1: 0.6603 - false_negatives_1: 136848.8594 - false_positives_1: 51191.1445 - loss: 0.4330 - precision_1: 0.6536 - recall_1: 0.4080 - val_binary_io_u_1: 0.4846 - val_false_negatives_1: 7013.0000 - val_false_positives_1: 0.0000e+00 - val_loss: 0.4353 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 7s/step - binary_io_u_1: 0.6740 - false_negatives_1: 132835.2656 - false_positives_1: 57147.9102 - loss: 0.4061 - precision_1: 0.6567 - recall_1: 0.4703 - val_binary_io_u_1: 0.4896 - val_false_negatives_1: 12989.0000 - val_false_positives_1: 0.0000e+00 - val_loss: 0.7287 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 7s/step - binary_io_u_1: 0.6831 - false_negatives_1: 124603.2891 - false_positives_1: 44216.0469 - loss: 0.3685 - precision_1: 0.7135 - recall_1: 0.4421 - val_binary_io_u_1: 0.4760 - val_false_negatives_1: 30208.0000 - val_false_positives_1: 0.0000e+00 - val_loss: 0.9995 - val_precision_1: 1.0000 - val_recall_1: 3.3103e-05\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
    "\n",
    "# Generate batches of augmented data from arrays (X_train and y_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=50, seed = 27)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=5, seed = 82)\n",
    " \n",
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return 1 - (numerator + 1) / (denominator + 1)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                   loss=dice_loss, \n",
    "                   metrics=[metrics.Precision(),\n",
    "                            metrics.Recall(),\n",
    "                            metrics.FalsePositives(),\n",
    "                            metrics.FalseNegatives(),\n",
    "                            metrics.BinaryIoU()])\n",
    "# Train the model using the fit_generator function\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch= 21,  \n",
    "    epochs=10, \n",
    "    validation_data=val_generator,\n",
    "    validation_steps= 6 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print( len(val_generator) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masg.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, image)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the image to a NumPy array with the expected shape\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43minput_image\u001b[49m)\n\u001b[0;32m     10\u001b[0m prediction\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'CFD_001.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.resize(image, (112, 112))  # Resize to match your model's input shape\n",
    "# image = image / 255.0  # Normalize the pixel values\n",
    "cv2.imwrite( \"asg.jpg\", image)\n",
    "# Convert the image to a NumPy array with the expected shape\n",
    "# input_image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
    "prediction = model.predict(input_image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mprediction\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m      6\u001b[0m prediction_2d \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction *= 255\n",
    "\n",
    "prediction_2d = prediction.squeeze()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(prediction_2d, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
